{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 18s 0us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 13s 9ms/step - loss: 1.6783 - acc: 0.3925 - val_loss: 1.4981 - val_acc: 0.4460\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4299 - acc: 0.4876 - val_loss: 1.2676 - val_acc: 0.5454\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3062 - acc: 0.5368 - val_loss: 1.2038 - val_acc: 0.5848\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2107 - acc: 0.5735 - val_loss: 1.0738 - val_acc: 0.6237\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1424 - acc: 0.5967 - val_loss: 1.0057 - val_acc: 0.6538\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0870 - acc: 0.6205 - val_loss: 0.9909 - val_acc: 0.6574\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0389 - acc: 0.6381 - val_loss: 0.9435 - val_acc: 0.6748\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0074 - acc: 0.6473 - val_loss: 1.0218 - val_acc: 0.6472\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9689 - acc: 0.6655 - val_loss: 1.0303 - val_acc: 0.6477\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9407 - acc: 0.6717 - val_loss: 0.9113 - val_acc: 0.6887\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9126 - acc: 0.6839 - val_loss: 0.9957 - val_acc: 0.6552\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8941 - acc: 0.6910 - val_loss: 0.9136 - val_acc: 0.6957\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8749 - acc: 0.6979 - val_loss: 0.8629 - val_acc: 0.7069\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8547 - acc: 0.7060 - val_loss: 0.8148 - val_acc: 0.7257\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8366 - acc: 0.7111 - val_loss: 0.8729 - val_acc: 0.7054\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8239 - acc: 0.7143 - val_loss: 0.8456 - val_acc: 0.7168\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8043 - acc: 0.7228 - val_loss: 0.7188 - val_acc: 0.7523\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7899 - acc: 0.7287 - val_loss: 0.7569 - val_acc: 0.7435\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7799 - acc: 0.7300 - val_loss: 0.7990 - val_acc: 0.7283\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7677 - acc: 0.7363 - val_loss: 0.7196 - val_acc: 0.7506\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7582 - acc: 0.7384 - val_loss: 0.7275 - val_acc: 0.7512\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.7453 - acc: 0.7425 - val_loss: 0.7491 - val_acc: 0.7450\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.7343 - acc: 0.7462 - val_loss: 0.7522 - val_acc: 0.7461\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7242 - acc: 0.7511 - val_loss: 0.7034 - val_acc: 0.7653\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7206 - acc: 0.7513 - val_loss: 0.7157 - val_acc: 0.7573\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7104 - acc: 0.7553 - val_loss: 0.7551 - val_acc: 0.7478\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7005 - acc: 0.7573 - val_loss: 0.6899 - val_acc: 0.7680\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6926 - acc: 0.7638 - val_loss: 0.7267 - val_acc: 0.7566\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.6895 - acc: 0.7618 - val_loss: 0.7224 - val_acc: 0.7557\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6829 - acc: 0.7648 - val_loss: 0.7088 - val_acc: 0.7618\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6713 - acc: 0.7682 - val_loss: 0.6736 - val_acc: 0.7754\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6674 - acc: 0.7705 - val_loss: 0.6723 - val_acc: 0.7747\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6647 - acc: 0.7725 - val_loss: 0.6935 - val_acc: 0.7723\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6563 - acc: 0.7754 - val_loss: 0.6697 - val_acc: 0.7744\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6523 - acc: 0.7755 - val_loss: 0.6558 - val_acc: 0.7817\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6416 - acc: 0.7773 - val_loss: 0.6665 - val_acc: 0.7745\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6414 - acc: 0.7792 - val_loss: 0.6738 - val_acc: 0.7745\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6327 - acc: 0.7811 - val_loss: 0.6960 - val_acc: 0.7707\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6260 - acc: 0.7851 - val_loss: 0.6153 - val_acc: 0.7941\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6242 - acc: 0.7834 - val_loss: 0.6621 - val_acc: 0.7813\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6187 - acc: 0.7858 - val_loss: 0.6374 - val_acc: 0.7882\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6144 - acc: 0.7879 - val_loss: 0.6665 - val_acc: 0.7823\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6127 - acc: 0.7880 - val_loss: 0.6175 - val_acc: 0.7978\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6068 - acc: 0.7910 - val_loss: 0.6332 - val_acc: 0.7885\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.6008 - acc: 0.7922 - val_loss: 0.6338 - val_acc: 0.7922\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5969 - acc: 0.7942 - val_loss: 0.6663 - val_acc: 0.7807\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5918 - acc: 0.7934 - val_loss: 0.6220 - val_acc: 0.7951\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5917 - acc: 0.7957 - val_loss: 0.6450 - val_acc: 0.7840\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5858 - acc: 0.7969 - val_loss: 0.6460 - val_acc: 0.7877\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5842 - acc: 0.7962 - val_loss: 0.7197 - val_acc: 0.7650\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5841 - acc: 0.7966 - val_loss: 0.6520 - val_acc: 0.7885\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5767 - acc: 0.8004 - val_loss: 0.6128 - val_acc: 0.7975\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5761 - acc: 0.8002 - val_loss: 0.6729 - val_acc: 0.7847\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5682 - acc: 0.8047 - val_loss: 0.6457 - val_acc: 0.7893\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5684 - acc: 0.8028 - val_loss: 0.5911 - val_acc: 0.8034\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5662 - acc: 0.8061 - val_loss: 0.6340 - val_acc: 0.7933\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5555 - acc: 0.8074 - val_loss: 0.6262 - val_acc: 0.7915\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5590 - acc: 0.8075 - val_loss: 0.6112 - val_acc: 0.8015\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5550 - acc: 0.8071 - val_loss: 0.5822 - val_acc: 0.8080\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5512 - acc: 0.8087 - val_loss: 0.6079 - val_acc: 0.8006\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5534 - acc: 0.8072 - val_loss: 0.6052 - val_acc: 0.8041\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5469 - acc: 0.8101 - val_loss: 0.6249 - val_acc: 0.7984\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5430 - acc: 0.8118 - val_loss: 0.5989 - val_acc: 0.8045\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5414 - acc: 0.8123 - val_loss: 0.6015 - val_acc: 0.8045\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5386 - acc: 0.8149 - val_loss: 0.6269 - val_acc: 0.8004\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5347 - acc: 0.8152 - val_loss: 0.6193 - val_acc: 0.8009\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5303 - acc: 0.8171 - val_loss: 0.6217 - val_acc: 0.8023\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5262 - acc: 0.8189 - val_loss: 0.6173 - val_acc: 0.8027\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5328 - acc: 0.8165 - val_loss: 0.5983 - val_acc: 0.8096\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5209 - acc: 0.8210 - val_loss: 0.6023 - val_acc: 0.8071\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5252 - acc: 0.8168 - val_loss: 0.6625 - val_acc: 0.7886\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5220 - acc: 0.8191 - val_loss: 0.5950 - val_acc: 0.8043\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5199 - acc: 0.8212 - val_loss: 0.6539 - val_acc: 0.7864\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5164 - acc: 0.8233 - val_loss: 0.6014 - val_acc: 0.8068\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5164 - acc: 0.8194 - val_loss: 0.6323 - val_acc: 0.7930\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5093 - acc: 0.8231 - val_loss: 0.5876 - val_acc: 0.8055\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5106 - acc: 0.8236 - val_loss: 0.5936 - val_acc: 0.8123\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5091 - acc: 0.8251 - val_loss: 0.5808 - val_acc: 0.8099\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5059 - acc: 0.8244 - val_loss: 0.5864 - val_acc: 0.8116\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5055 - acc: 0.8242 - val_loss: 0.5967 - val_acc: 0.8079\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5050 - acc: 0.8245 - val_loss: 0.6182 - val_acc: 0.8015\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5022 - acc: 0.8262 - val_loss: 0.6105 - val_acc: 0.8049\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5008 - acc: 0.8269 - val_loss: 0.6344 - val_acc: 0.7998\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4956 - acc: 0.8280 - val_loss: 0.6144 - val_acc: 0.8065\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4993 - acc: 0.8277 - val_loss: 0.6009 - val_acc: 0.8148\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4912 - acc: 0.8307 - val_loss: 0.5571 - val_acc: 0.8152\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4926 - acc: 0.8293 - val_loss: 0.5701 - val_acc: 0.8160\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4925 - acc: 0.8293 - val_loss: 0.6027 - val_acc: 0.8090\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4882 - acc: 0.8303 - val_loss: 0.6011 - val_acc: 0.8098\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.4863 - acc: 0.8321 - val_loss: 0.5757 - val_acc: 0.8169\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4817 - acc: 0.8333 - val_loss: 0.5818 - val_acc: 0.8141\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4865 - acc: 0.8310 - val_loss: 0.5753 - val_acc: 0.8128\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4817 - acc: 0.8309 - val_loss: 0.5976 - val_acc: 0.8123\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4793 - acc: 0.8353 - val_loss: 0.6217 - val_acc: 0.8092\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4806 - acc: 0.8344 - val_loss: 0.6017 - val_acc: 0.8089\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4741 - acc: 0.8359 - val_loss: 0.6135 - val_acc: 0.8053\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4750 - acc: 0.8354 - val_loss: 0.6166 - val_acc: 0.8045\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.4734 - acc: 0.8357 - val_loss: 0.5679 - val_acc: 0.8150\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.4746 - acc: 0.8356 - val_loss: 0.5696 - val_acc: 0.8207\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4731 - acc: 0.8368 - val_loss: 0.5823 - val_acc: 0.8109\n",
      "Saved trained model at /home/amir/code/personal/numpy-mlp/notebooks/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "Test loss: 0.5822540023803711\n",
      "Test accuracy: 0.8109\n"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with  L2 regularization on Dense and Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 2.2331 - acc: 0.3855 - val_loss: 1.8988 - val_acc: 0.4738\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.7877 - acc: 0.4802 - val_loss: 1.5657 - val_acc: 0.5426\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.5915 - acc: 0.5250 - val_loss: 1.4408 - val_acc: 0.5748\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.4685 - acc: 0.5581 - val_loss: 1.3241 - val_acc: 0.6034\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3879 - acc: 0.5831 - val_loss: 1.3025 - val_acc: 0.6154\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.3270 - acc: 0.5982 - val_loss: 1.2010 - val_acc: 0.6488\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2825 - acc: 0.6153 - val_loss: 1.2396 - val_acc: 0.6380\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2435 - acc: 0.6287 - val_loss: 1.1397 - val_acc: 0.6654\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2079 - acc: 0.6377 - val_loss: 1.1349 - val_acc: 0.6637\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1846 - acc: 0.6475 - val_loss: 1.0489 - val_acc: 0.6949\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1581 - acc: 0.6573 - val_loss: 1.0770 - val_acc: 0.6865\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1342 - acc: 0.6642 - val_loss: 1.0478 - val_acc: 0.7014\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.1095 - acc: 0.6760 - val_loss: 1.1014 - val_acc: 0.6847\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0900 - acc: 0.6789 - val_loss: 1.0181 - val_acc: 0.7127\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0753 - acc: 0.6894 - val_loss: 0.9999 - val_acc: 0.7171\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0643 - acc: 0.6898 - val_loss: 0.9608 - val_acc: 0.7269\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0510 - acc: 0.6977 - val_loss: 0.9604 - val_acc: 0.7275\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0356 - acc: 0.7015 - val_loss: 1.0115 - val_acc: 0.7157\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0241 - acc: 0.7050 - val_loss: 0.9987 - val_acc: 0.7130\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0086 - acc: 0.7097 - val_loss: 0.9892 - val_acc: 0.7190\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0002 - acc: 0.7129 - val_loss: 0.9708 - val_acc: 0.7252\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9918 - acc: 0.7162 - val_loss: 0.9389 - val_acc: 0.7400\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9846 - acc: 0.7164 - val_loss: 0.9355 - val_acc: 0.7347\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9727 - acc: 0.7238 - val_loss: 1.0431 - val_acc: 0.7101\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9642 - acc: 0.7266 - val_loss: 0.9095 - val_acc: 0.7457\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9584 - acc: 0.7287 - val_loss: 0.9307 - val_acc: 0.7447\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9459 - acc: 0.7318 - val_loss: 0.9121 - val_acc: 0.7447\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9459 - acc: 0.7329 - val_loss: 0.9196 - val_acc: 0.7458\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9353 - acc: 0.7362 - val_loss: 0.9343 - val_acc: 0.7428\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9252 - acc: 0.7406 - val_loss: 1.0168 - val_acc: 0.7187\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9179 - acc: 0.7439 - val_loss: 0.8968 - val_acc: 0.7534\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9136 - acc: 0.7432 - val_loss: 0.8693 - val_acc: 0.7616\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9090 - acc: 0.7459 - val_loss: 0.8481 - val_acc: 0.7687\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9046 - acc: 0.7462 - val_loss: 0.8668 - val_acc: 0.7590\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8973 - acc: 0.7487 - val_loss: 0.8913 - val_acc: 0.7528\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8936 - acc: 0.7483 - val_loss: 0.8541 - val_acc: 0.7700\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8857 - acc: 0.7541 - val_loss: 0.8765 - val_acc: 0.7597\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8804 - acc: 0.7546 - val_loss: 0.8591 - val_acc: 0.7679\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8789 - acc: 0.7534 - val_loss: 0.8674 - val_acc: 0.7615\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8737 - acc: 0.7598 - val_loss: 0.8535 - val_acc: 0.7672\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8684 - acc: 0.7582 - val_loss: 0.8374 - val_acc: 0.7715\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8658 - acc: 0.7620 - val_loss: 0.8800 - val_acc: 0.7604\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8585 - acc: 0.7637 - val_loss: 0.9067 - val_acc: 0.7549\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.8554 - acc: 0.7638 - val_loss: 0.8195 - val_acc: 0.7823\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8500 - acc: 0.7673 - val_loss: 0.7885 - val_acc: 0.7904\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8492 - acc: 0.7667 - val_loss: 0.8314 - val_acc: 0.7757\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8443 - acc: 0.7692 - val_loss: 0.7977 - val_acc: 0.7886\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8384 - acc: 0.7695 - val_loss: 0.8277 - val_acc: 0.7752\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8368 - acc: 0.7692 - val_loss: 0.8532 - val_acc: 0.7704\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8292 - acc: 0.7742 - val_loss: 0.8241 - val_acc: 0.7752\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8303 - acc: 0.7733 - val_loss: 0.8710 - val_acc: 0.7618\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8240 - acc: 0.7747 - val_loss: 0.8188 - val_acc: 0.7814\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8206 - acc: 0.7750 - val_loss: 0.8621 - val_acc: 0.7670\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8151 - acc: 0.7764 - val_loss: 0.7879 - val_acc: 0.7873\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8156 - acc: 0.7763 - val_loss: 0.8650 - val_acc: 0.7625\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8145 - acc: 0.7764 - val_loss: 0.7924 - val_acc: 0.7875\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8083 - acc: 0.7791 - val_loss: 0.8018 - val_acc: 0.7865\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8056 - acc: 0.7809 - val_loss: 0.7635 - val_acc: 0.8003\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8023 - acc: 0.7821 - val_loss: 0.7942 - val_acc: 0.7861\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7982 - acc: 0.7817 - val_loss: 0.8139 - val_acc: 0.7837\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7944 - acc: 0.7833 - val_loss: 0.8129 - val_acc: 0.7814\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7933 - acc: 0.7838 - val_loss: 0.7783 - val_acc: 0.7908\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7866 - acc: 0.7872 - val_loss: 0.7791 - val_acc: 0.7900\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7836 - acc: 0.7874 - val_loss: 0.7941 - val_acc: 0.7842\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7856 - acc: 0.7864 - val_loss: 0.7841 - val_acc: 0.7907\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7838 - acc: 0.7891 - val_loss: 0.8128 - val_acc: 0.7868\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7779 - acc: 0.7892 - val_loss: 0.8226 - val_acc: 0.7800\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7781 - acc: 0.7895 - val_loss: 0.7446 - val_acc: 0.8013\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7723 - acc: 0.7910 - val_loss: 0.8024 - val_acc: 0.7853\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7728 - acc: 0.7935 - val_loss: 0.7599 - val_acc: 0.7981\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7650 - acc: 0.7964 - val_loss: 0.7658 - val_acc: 0.7979\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7677 - acc: 0.7953 - val_loss: 0.7995 - val_acc: 0.7909\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7623 - acc: 0.7959 - val_loss: 0.7760 - val_acc: 0.7953\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7603 - acc: 0.7952 - val_loss: 0.7818 - val_acc: 0.7904\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7634 - acc: 0.7946 - val_loss: 0.7712 - val_acc: 0.7924\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7575 - acc: 0.7960 - val_loss: 0.7702 - val_acc: 0.7938\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7558 - acc: 0.7972 - val_loss: 0.7997 - val_acc: 0.7896\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7545 - acc: 0.7974 - val_loss: 0.7631 - val_acc: 0.7979\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7513 - acc: 0.7992 - val_loss: 0.8080 - val_acc: 0.7889\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7519 - acc: 0.7977 - val_loss: 0.8312 - val_acc: 0.7818\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7480 - acc: 0.7982 - val_loss: 0.7587 - val_acc: 0.7945\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7508 - acc: 0.7980 - val_loss: 0.8120 - val_acc: 0.7854\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7393 - acc: 0.8010 - val_loss: 0.7664 - val_acc: 0.7992\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7438 - acc: 0.8011 - val_loss: 0.7621 - val_acc: 0.7977\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7379 - acc: 0.8019 - val_loss: 0.7795 - val_acc: 0.7933\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7381 - acc: 0.8028 - val_loss: 0.7743 - val_acc: 0.7957\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7362 - acc: 0.8038 - val_loss: 0.7768 - val_acc: 0.7935\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7365 - acc: 0.8039 - val_loss: 0.7705 - val_acc: 0.7951\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7339 - acc: 0.8057 - val_loss: 0.7431 - val_acc: 0.8052\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7321 - acc: 0.8048 - val_loss: 0.7927 - val_acc: 0.7890\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7296 - acc: 0.8049 - val_loss: 0.7563 - val_acc: 0.8059\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7265 - acc: 0.8062 - val_loss: 0.7423 - val_acc: 0.8039\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7255 - acc: 0.8082 - val_loss: 0.7618 - val_acc: 0.8015\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7244 - acc: 0.8079 - val_loss: 0.7550 - val_acc: 0.8012\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7180 - acc: 0.8101 - val_loss: 0.7741 - val_acc: 0.8003\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7202 - acc: 0.8081 - val_loss: 0.7584 - val_acc: 0.8034\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7178 - acc: 0.8116 - val_loss: 0.7172 - val_acc: 0.8127\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.7194 - acc: 0.8076 - val_loss: 0.7228 - val_acc: 0.8119\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7154 - acc: 0.8106 - val_loss: 0.7743 - val_acc: 0.7925\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7129 - acc: 0.8112 - val_loss: 0.7942 - val_acc: 0.7899\n",
      "Saved trained model at /home/amir/code/personal/numpy-mlp/notebooks/saved_models/keras_cifar10_trained_model_l2.h5 \n",
      "10000/10000 [==============================] - 1s 57us/step\n",
      "Test loss: 0.7942470280647278\n",
      "Test accuracy: 0.7899\n"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from numpy.random import seed\n",
    "from keras import regularizers\n",
    "\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model_l2.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.001), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history_2 = model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history_2 = model_2.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores_2 = model_2.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_2[0])\n",
    "print('Test accuracy:', scores_2[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 2.1446 - acc: 0.3679 - val_loss: 1.8793 - val_acc: 0.4341\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.7357 - acc: 0.4640 - val_loss: 1.5135 - val_acc: 0.5349\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.5470 - acc: 0.5107 - val_loss: 1.4124 - val_acc: 0.5439\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.4200 - acc: 0.5432 - val_loss: 1.2644 - val_acc: 0.5969\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.3357 - acc: 0.5676 - val_loss: 1.2338 - val_acc: 0.5915\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2787 - acc: 0.5842 - val_loss: 1.2200 - val_acc: 0.6022\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.2279 - acc: 0.5995 - val_loss: 1.1516 - val_acc: 0.6286\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1886 - acc: 0.6149 - val_loss: 1.1709 - val_acc: 0.6198\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1560 - acc: 0.6207 - val_loss: 1.1747 - val_acc: 0.6242\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.1259 - acc: 0.6341 - val_loss: 1.1192 - val_acc: 0.6387\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0978 - acc: 0.6437 - val_loss: 1.2005 - val_acc: 0.6222\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0775 - acc: 0.6490 - val_loss: 0.9711 - val_acc: 0.6887\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0560 - acc: 0.6569 - val_loss: 0.9970 - val_acc: 0.6781\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0346 - acc: 0.6647 - val_loss: 0.9202 - val_acc: 0.7065\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.0145 - acc: 0.6721 - val_loss: 0.9345 - val_acc: 0.6994\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9951 - acc: 0.6768 - val_loss: 0.9789 - val_acc: 0.6911\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9803 - acc: 0.6816 - val_loss: 1.0147 - val_acc: 0.6810\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9646 - acc: 0.6876 - val_loss: 0.8966 - val_acc: 0.7125\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9502 - acc: 0.6926 - val_loss: 0.8781 - val_acc: 0.7238\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9334 - acc: 0.6996 - val_loss: 0.9156 - val_acc: 0.7123\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9270 - acc: 0.7014 - val_loss: 0.8358 - val_acc: 0.7399\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9126 - acc: 0.7069 - val_loss: 0.9095 - val_acc: 0.7094\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9049 - acc: 0.7107 - val_loss: 0.8646 - val_acc: 0.7321\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8950 - acc: 0.7144 - val_loss: 0.8391 - val_acc: 0.7339\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8865 - acc: 0.7167 - val_loss: 0.8372 - val_acc: 0.7418\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8769 - acc: 0.7214 - val_loss: 0.8708 - val_acc: 0.7231\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8693 - acc: 0.7221 - val_loss: 0.8321 - val_acc: 0.7400\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8626 - acc: 0.7255 - val_loss: 0.8661 - val_acc: 0.7237\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8525 - acc: 0.7296 - val_loss: 0.8349 - val_acc: 0.7396\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8446 - acc: 0.7315 - val_loss: 0.7913 - val_acc: 0.7539\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8378 - acc: 0.7340 - val_loss: 0.7988 - val_acc: 0.7580\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8348 - acc: 0.7356 - val_loss: 0.8318 - val_acc: 0.7424\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8270 - acc: 0.7384 - val_loss: 0.7945 - val_acc: 0.7537\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8185 - acc: 0.7404 - val_loss: 0.7941 - val_acc: 0.7539\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8143 - acc: 0.7423 - val_loss: 0.8153 - val_acc: 0.7497\n",
      "Epoch 36/100\n",
      "1378/1563 [=========================>....] - ETA: 1s - loss: 0.8069 - acc: 0.7466"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ebc04d08df9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                         workers=4)\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Save model and weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from numpy.random import seed\n",
    "from keras import regularizers\n",
    "\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model_l2.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(Conv2D(32, (3, 3)))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_3.add(Conv2D(64, (3, 3)))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(Conv2D(64, (3, 3)))\n",
    "model_3.add(Activation('relu'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_3.add(Dense(num_classes))\n",
    "model_3.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history_3 = model_3.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history_3 = model_3.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores_3 = model_3.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', model_3[0])\n",
    "print('Test accuracy:', model_3[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
