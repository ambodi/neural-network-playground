{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not using L2 regularization is better than L2 regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case of MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are having a study group for Stockholm AI soon. During a discussion with my friend, Carl, I told him that it is common to use L2 regularization and L1 at the same time. We then continued the dicussion and I started to wonder: would it actually perform the performance of a simple MLP on a simple data like MNIST? But then I started a simple experiment: let's compare the performance of an MLP with no regularization with L1, L2 and a combination of L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything else, let's ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow\n",
    "\n",
    "numpy.random.seed(7)\n",
    "tensorflow.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually like to import everything first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple MLP from Keras' examples running MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple MLP with no regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the simplest MLP one can find. The point with this MLP is not that it should be the state-of-the-art among MLPs, since the model is not going to change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2198 - acc: 0.9315 - val_loss: 0.1033 - val_acc: 0.9667\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0822 - acc: 0.9745 - val_loss: 0.1036 - val_acc: 0.9672\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0541 - acc: 0.9832 - val_loss: 0.0741 - val_acc: 0.9797\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0391 - acc: 0.9879 - val_loss: 0.0712 - val_acc: 0.9804\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0292 - acc: 0.9909 - val_loss: 0.0799 - val_acc: 0.9807\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0238 - acc: 0.9929 - val_loss: 0.0948 - val_acc: 0.9784\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0195 - acc: 0.9940 - val_loss: 0.0946 - val_acc: 0.9804\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0159 - acc: 0.9950 - val_loss: 0.0971 - val_acc: 0.9804\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 0.1136 - val_acc: 0.9785\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.1311 - val_acc: 0.9792\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 0.1210 - val_acc: 0.9800\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.1357 - val_acc: 0.9803\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.1187 - val_acc: 0.9824\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 0.1250 - val_acc: 0.9826\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.1345 - val_acc: 0.9823\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.1385 - val_acc: 0.9819\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.1508 - val_acc: 0.9809\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1508 - val_acc: 0.9817\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.1682 - val_acc: 0.9786\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.1374 - val_acc: 0.9830\n",
      "Test loss: 0.13740726012259066\n",
      "Test accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we all know that L2 regularization helps generalizatin of a neural network or any ML model, right? I thought so too. So I started checking the usual parameter for L2 regulariztion, namely 0.001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.6337 - acc: 0.9214 - val_loss: 0.3201 - val_acc: 0.9552\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.2652 - acc: 0.9608 - val_loss: 0.2342 - val_acc: 0.9627\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.2068 - acc: 0.9675 - val_loss: 0.2404 - val_acc: 0.9535\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1785 - acc: 0.9715 - val_loss: 0.1863 - val_acc: 0.9640\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1617 - acc: 0.9734 - val_loss: 0.1560 - val_acc: 0.9756\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1517 - acc: 0.9748 - val_loss: 0.1821 - val_acc: 0.9648\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.1425 - acc: 0.9764 - val_loss: 0.1485 - val_acc: 0.9740\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1366 - acc: 0.9771 - val_loss: 0.1797 - val_acc: 0.9616\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 58us/step - loss: 0.1308 - acc: 0.9779 - val_loss: 0.1454 - val_acc: 0.9731\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1262 - acc: 0.9788 - val_loss: 0.1358 - val_acc: 0.9751\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1242 - acc: 0.9792 - val_loss: 0.1469 - val_acc: 0.9710\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1210 - acc: 0.9797 - val_loss: 0.1486 - val_acc: 0.9710\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1186 - acc: 0.9800 - val_loss: 0.1715 - val_acc: 0.9627\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1164 - acc: 0.9805 - val_loss: 0.1294 - val_acc: 0.9756\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1147 - acc: 0.9802 - val_loss: 0.1252 - val_acc: 0.9766\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1138 - acc: 0.9809 - val_loss: 0.1844 - val_acc: 0.9597\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1126 - acc: 0.9811 - val_loss: 0.1204 - val_acc: 0.9788\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1112 - acc: 0.9809 - val_loss: 0.1269 - val_acc: 0.9765\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1112 - acc: 0.9812 - val_loss: 0.1365 - val_acc: 0.9742\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1088 - acc: 0.9815 - val_loss: 0.1565 - val_acc: 0.9704\n",
      "Test loss: 0.15646419671773912\n",
      "Test accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=regularizers.l2(0.001)))\n",
    "# model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "# model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model_2.summary()\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_2 = model_2.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score_2[0])\n",
    "print('Test accuracy:', score_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was confused a bit. How can the network with L2 regularization perform worse than the one without it? Isn't one the promise of regularization is to help generalization? Could it be that the L2 coefficient needs more tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamter optimization of L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_206 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.9411116557115164\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 176.0764 - acc: 0.1344 - val_loss: 3.1220 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1216 - acc: 0.1112 - val_loss: 3.1211 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 3.1213 - acc: 0.1118 - val_loss: 3.1211 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 3.1211 - acc: 0.1116 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1211 - acc: 0.1124 - val_loss: 3.1208 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 3.1210 - acc: 0.1124 - val_loss: 3.1208 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 3.1210 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 3.1210 - acc: 0.1124 - val_loss: 3.1208 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 3.1210 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 3.1210 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1206 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 3.1209 - acc: 0.1124 - val_loss: 3.1207 - val_acc: 0.1135\n",
      "Test loss: 3.1206957168579104\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_209 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 3.08375499943383\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 111.0423 - acc: 0.1465 - val_loss: 2.8129 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8133 - acc: 0.1115 - val_loss: 2.8132 - val_acc: 0.1028\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8131 - acc: 0.1114 - val_loss: 2.8127 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.8129 - acc: 0.1120 - val_loss: 2.8129 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8130 - acc: 0.1124 - val_loss: 2.8126 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8126 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8126 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8127 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8127 - acc: 0.1124 - val_loss: 2.8126 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8128 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8127 - acc: 0.1124 - val_loss: 2.8125 - val_acc: 0.1135\n",
      "Test loss: 2.8125485736846922\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_212 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.837531232093046\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 6s 105us/step - loss: 172.9070 - acc: 0.1395 - val_loss: 3.1044 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1044 - acc: 0.1121 - val_loss: 3.1038 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1042 - acc: 0.1122 - val_loss: 3.1037 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1040 - acc: 0.1124 - val_loss: 3.1037 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1039 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1039 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1039 - acc: 0.1124 - val_loss: 3.1035 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1039 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1034 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1035 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1035 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1035 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1035 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1035 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 3.1038 - acc: 0.1124 - val_loss: 3.1036 - val_acc: 0.1135\n",
      "Test loss: 3.103575607299805\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_215 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.429650589851541\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 158.4353 - acc: 0.1377 - val_loss: 3.0371 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0366 - acc: 0.1109 - val_loss: 3.0360 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0365 - acc: 0.1124 - val_loss: 3.0361 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0363 - acc: 0.1124 - val_loss: 3.0362 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 3.0362 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0362 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0359 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0359 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0359 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0361 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0360 - acc: 0.1124 - val_loss: 3.0358 - val_acc: 0.1135\n",
      "Test loss: 3.0358422523498536\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_218 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 3.6017958480174177\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 128.9279 - acc: 0.1428 - val_loss: 2.9008 - val_acc: 0.0958\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8994 - acc: 0.1110 - val_loss: 2.8988 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8991 - acc: 0.1118 - val_loss: 2.8991 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8990 - acc: 0.1124 - val_loss: 2.8986 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8989 - acc: 0.1124 - val_loss: 2.8987 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8989 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8989 - acc: 0.1124 - val_loss: 2.8987 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8989 - acc: 0.1124 - val_loss: 2.8986 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8986 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8986 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8986 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8986 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.8988 - acc: 0.1124 - val_loss: 2.8985 - val_acc: 0.1135\n",
      "Test loss: 2.8984878246307373\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_221 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 3.1853644572780424\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 114.3722 - acc: 0.1442 - val_loss: 2.8315 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 2.8304 - acc: 0.1111 - val_loss: 2.8298 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 2.8300 - acc: 0.1114 - val_loss: 2.8298 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.8300 - acc: 0.1124 - val_loss: 2.8295 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.8298 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.8298 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8296 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8296 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8295 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8295 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8295 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8297 - acc: 0.1124 - val_loss: 2.8294 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.8296 - acc: 0.1124 - val_loss: 2.8296 - val_acc: 0.1135\n",
      "Test loss: 2.829578931427002\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_224 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.937558037192623\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 176.4252 - acc: 0.1392 - val_loss: 3.1206 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1210 - acc: 0.1115 - val_loss: 3.1206 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1207 - acc: 0.1119 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1204 - acc: 0.1118 - val_loss: 3.1203 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 3.1205 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1204 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1204 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1204 - acc: 0.1124 - val_loss: 3.1202 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1204 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1204 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1204 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.1203 - acc: 0.1124 - val_loss: 3.1201 - val_acc: 0.1135\n",
      "Test loss: 3.120051607131958\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_227 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.182323534660662\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 149.5270 - acc: 0.1411 - val_loss: 2.9956 - val_acc: 0.1028\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9957 - acc: 0.1109 - val_loss: 2.9955 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9954 - acc: 0.1123 - val_loss: 2.9951 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9952 - acc: 0.1124 - val_loss: 2.9951 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9952 - acc: 0.1123 - val_loss: 2.9950 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9949 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9949 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9949 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9947 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9951 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.9950 - acc: 0.1124 - val_loss: 2.9948 - val_acc: 0.1135\n",
      "Test loss: 2.9948267265319823\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_230 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.642319795996836\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 165.9693 - acc: 0.1391 - val_loss: 3.0723 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0722 - acc: 0.1107 - val_loss: 3.0725 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0718 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0715 - acc: 0.1121 - val_loss: 3.0712 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0714 - acc: 0.1124 - val_loss: 3.0715 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.0714 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.0714 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0714 - acc: 0.1124 - val_loss: 3.0710 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0710 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0710 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 3.0713 - acc: 0.1124 - val_loss: 3.0711 - val_acc: 0.1135\n",
      "Test loss: 3.071059013748169\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_233 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_234 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.7515436302009422\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 28.6785 - acc: 0.1538 - val_loss: 2.4293 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.4267 - acc: 0.1113 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4264 - acc: 0.1120 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4262 - acc: 0.1119 - val_loss: 2.4259 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.4261 - acc: 0.1124 - val_loss: 2.4258 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4261 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4261 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4260 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4260 - acc: 0.1124 - val_loss: 2.4256 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4260 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4256 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4258 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4257 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4256 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.4259 - acc: 0.1124 - val_loss: 2.4256 - val_acc: 0.1135\n",
      "Test loss: 2.425634540557861\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_236 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 3.116967350481192\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 112.0086 - acc: 0.1394 - val_loss: 2.8194 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8189 - acc: 0.1094 - val_loss: 2.8185 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8188 - acc: 0.1109 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8186 - acc: 0.1124 - val_loss: 2.8183 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8185 - acc: 0.1123 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8184 - acc: 0.1124 - val_loss: 2.8183 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8184 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8184 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8182 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8180 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8180 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8181 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8180 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8180 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8180 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8183 - acc: 0.1124 - val_loss: 2.8180 - val_acc: 0.1135\n",
      "Test loss: 2.818022216415405\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_239 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 3.403135835172795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 122.1014 - acc: 0.1461 - val_loss: 2.8670 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8666 - acc: 0.1101 - val_loss: 2.8657 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8663 - acc: 0.1104 - val_loss: 2.8658 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8661 - acc: 0.1122 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8660 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8659 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8659 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8659 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8659 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8655 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.8658 - acc: 0.1124 - val_loss: 2.8656 - val_acc: 0.1135\n",
      "Test loss: 2.8655641693115235\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_242 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.569291271405376\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 163.3485 - acc: 0.1465 - val_loss: 3.0596 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0599 - acc: 0.1101 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0597 - acc: 0.1120 - val_loss: 3.0594 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0594 - acc: 0.1116 - val_loss: 3.0593 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0595 - acc: 0.1124 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0594 - acc: 0.1124 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0594 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0589 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0591 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0592 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0593 - acc: 0.1124 - val_loss: 3.0590 - val_acc: 0.1135\n",
      "Test loss: 3.058990214538574\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_245 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 1.3469262280880363\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 49.6794 - acc: 0.1476 - val_loss: 2.5247 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5253 - acc: 0.1113 - val_loss: 2.5249 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5251 - acc: 0.1113 - val_loss: 2.5246 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5249 - acc: 0.1122 - val_loss: 2.5247 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5249 - acc: 0.1124 - val_loss: 2.5248 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5248 - acc: 0.1124 - val_loss: 2.5246 - val_acc: 0.1135\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5248 - acc: 0.1124 - val_loss: 2.5245 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5248 - acc: 0.1124 - val_loss: 2.5246 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5248 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5246 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5245 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5245 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5245 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5247 - acc: 0.1124 - val_loss: 2.5244 - val_acc: 0.1135\n",
      "Test loss: 2.5244004684448242\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_248 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.4970770810778244\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 160.5477 - acc: 0.1408 - val_loss: 3.0480 - val_acc: 0.1028\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0479 - acc: 0.1112 - val_loss: 3.0474 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 3.0477 - acc: 0.1123 - val_loss: 3.0471 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0474 - acc: 0.1118 - val_loss: 3.0469 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0474 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0471 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0473 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0473 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0469 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0469 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0470 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0469 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0469 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 3.0472 - acc: 0.1124 - val_loss: 3.0469 - val_acc: 0.1135\n",
      "Test loss: 3.04693928565979\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_251 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.057065437333802\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 145.0751 - acc: 0.1373 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9749 - acc: 0.1119 - val_loss: 2.9742 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9746 - acc: 0.1115 - val_loss: 2.9743 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9745 - acc: 0.1120 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.9744 - acc: 0.1124 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9744 - acc: 0.1124 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9742 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9742 - acc: 0.1124 - val_loss: 2.9742 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9741 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.9742 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.9743 - acc: 0.1124 - val_loss: 2.9740 - val_acc: 0.1135\n",
      "Test loss: 2.974040071105957\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_254 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 1.5449635821779377\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 56.7622 - acc: 0.1493 - val_loss: 2.5580 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.5581 - acc: 0.1106 - val_loss: 2.5578 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5579 - acc: 0.1113 - val_loss: 2.5580 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5578 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5576 - acc: 0.1119 - val_loss: 2.5575 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5577 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5572 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5573 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5573 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5573 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5573 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 2.5576 - acc: 0.1124 - val_loss: 2.5573 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5575 - acc: 0.1124 - val_loss: 2.5572 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5575 - acc: 0.1124 - val_loss: 2.5572 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5575 - acc: 0.1124 - val_loss: 2.5573 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.5575 - acc: 0.1124 - val_loss: 2.5574 - val_acc: 0.1135\n",
      "Test loss: 2.5573690269470215\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_257 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_258 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 1.8035515293956061\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 65.6817 - acc: 0.1478 - val_loss: 2.6007 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6012 - acc: 0.1104 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6008 - acc: 0.1123 - val_loss: 2.6006 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6007 - acc: 0.1120 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6006 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6006 - acc: 0.1124 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6005 - acc: 0.1124 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6005 - acc: 0.1124 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6005 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6005 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6005 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6003 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6001 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6001 - val_acc: 0.1135\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6002 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6004 - acc: 0.1124 - val_loss: 2.6001 - val_acc: 0.1135\n",
      "Test loss: 2.600121851348877\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_260 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 4.882268556339547\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 174.2185 - acc: 0.1370 - val_loss: 3.1120 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 3.1118 - acc: 0.1113 - val_loss: 3.1116 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 3.1115 - acc: 0.1121 - val_loss: 3.1115 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 3.1114 - acc: 0.1118 - val_loss: 3.1111 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 3.1113 - acc: 0.1124 - val_loss: 3.1111 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1113 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 3.1113 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1113 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 3.1113 - acc: 0.1124 - val_loss: 3.1111 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 3.1113 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1109 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1109 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1109 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1110 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1109 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1109 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 3.1112 - acc: 0.1124 - val_loss: 3.1109 - val_acc: 0.1135\n",
      "Test loss: 3.110940944671631\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_263 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_265 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 2.157525539570511\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 78.2248 - acc: 0.1437 - val_loss: 2.6593 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 2.6598 - acc: 0.1111 - val_loss: 2.6591 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6597 - acc: 0.1121 - val_loss: 2.6590 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6593 - acc: 0.1124 - val_loss: 2.6593 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6593 - acc: 0.1124 - val_loss: 2.6591 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6591 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6590 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6588 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6592 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6590 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6590 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 2.6591 - acc: 0.1124 - val_loss: 2.6589 - val_acc: 0.1135\n",
      "Test loss: 2.6588534400939943\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_266 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_267 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_268 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 2.2475373252921016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 81.3187 - acc: 0.1375 - val_loss: 2.6761 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6746 - acc: 0.1111 - val_loss: 2.6744 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6745 - acc: 0.1121 - val_loss: 2.6741 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6744 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6742 - acc: 0.1124 - val_loss: 2.6739 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.6742 - acc: 0.1124 - val_loss: 2.6740 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6742 - acc: 0.1124 - val_loss: 2.6740 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6742 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6739 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6739 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6739 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.6741 - acc: 0.1124 - val_loss: 2.6738 - val_acc: 0.1135\n",
      "Test loss: 2.673814180755615\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_269 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_270 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.07455111637266487\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 4.0212 - acc: 0.7774 - val_loss: 0.9765 - val_acc: 0.8478\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.9115 - acc: 0.8522 - val_loss: 0.8573 - val_acc: 0.8479\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7939 - acc: 0.8682 - val_loss: 0.7327 - val_acc: 0.8839\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7299 - acc: 0.8782 - val_loss: 0.6757 - val_acc: 0.8898\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6888 - acc: 0.8848 - val_loss: 0.7575 - val_acc: 0.8588\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6648 - acc: 0.8883 - val_loss: 0.6453 - val_acc: 0.8972\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6401 - acc: 0.8923 - val_loss: 0.5868 - val_acc: 0.9089\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6256 - acc: 0.8950 - val_loss: 0.5716 - val_acc: 0.9107\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6102 - acc: 0.8984 - val_loss: 0.5693 - val_acc: 0.9068\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.5980 - acc: 0.8982 - val_loss: 0.5480 - val_acc: 0.9163\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.5860 - acc: 0.9014 - val_loss: 0.6325 - val_acc: 0.8831\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.5773 - acc: 0.9021 - val_loss: 0.5367 - val_acc: 0.9093\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.5699 - acc: 0.9028 - val_loss: 0.6578 - val_acc: 0.8678\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.5612 - acc: 0.9058 - val_loss: 0.6037 - val_acc: 0.8894\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.5540 - acc: 0.9068 - val_loss: 0.5931 - val_acc: 0.8958\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5491 - acc: 0.9047 - val_loss: 0.5784 - val_acc: 0.8905\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.5410 - acc: 0.9072 - val_loss: 0.5619 - val_acc: 0.8997\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.5383 - acc: 0.9061 - val_loss: 0.5058 - val_acc: 0.9164\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.5341 - acc: 0.9069 - val_loss: 0.4937 - val_acc: 0.9218\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.5263 - acc: 0.9093 - val_loss: 0.5230 - val_acc: 0.9072\n",
      "Test loss: 0.5229550045013428\n",
      "Test accuracy: 0.9072\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_272 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_274 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.010038202367633864\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 1.4189 - acc: 0.8760 - val_loss: 0.5247 - val_acc: 0.9219\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.4898 - acc: 0.9213 - val_loss: 0.4132 - val_acc: 0.9392\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.4063 - acc: 0.9365 - val_loss: 0.3679 - val_acc: 0.9422\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.3614 - acc: 0.9431 - val_loss: 0.3196 - val_acc: 0.9542\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.3321 - acc: 0.9476 - val_loss: 0.2956 - val_acc: 0.9612\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.3113 - acc: 0.9522 - val_loss: 0.2859 - val_acc: 0.9585\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.2982 - acc: 0.9531 - val_loss: 0.2761 - val_acc: 0.9596\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.2888 - acc: 0.9550 - val_loss: 0.2943 - val_acc: 0.9516\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.2796 - acc: 0.9553 - val_loss: 0.2847 - val_acc: 0.9522\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2740 - acc: 0.9566 - val_loss: 0.2569 - val_acc: 0.9595\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.2673 - acc: 0.9568 - val_loss: 0.2736 - val_acc: 0.9574\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.2650 - acc: 0.9572 - val_loss: 0.2905 - val_acc: 0.9491\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2611 - acc: 0.9578 - val_loss: 0.3046 - val_acc: 0.9422\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.2575 - acc: 0.9584 - val_loss: 0.2431 - val_acc: 0.9627\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2554 - acc: 0.9584 - val_loss: 0.2289 - val_acc: 0.9662\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2519 - acc: 0.9586 - val_loss: 0.2476 - val_acc: 0.9595\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.2495 - acc: 0.9583 - val_loss: 0.2445 - val_acc: 0.9569\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.2470 - acc: 0.9589 - val_loss: 0.2178 - val_acc: 0.9683\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2444 - acc: 0.9595 - val_loss: 0.2916 - val_acc: 0.9441\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2437 - acc: 0.9590 - val_loss: 0.3017 - val_acc: 0.9382\n",
      "Test loss: 0.30169162721633913\n",
      "Test accuracy: 0.9382\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_275 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_276 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.05698305536912378\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 3.3426 - acc: 0.7969 - val_loss: 0.9467 - val_acc: 0.8395\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.8381 - acc: 0.8624 - val_loss: 0.7355 - val_acc: 0.8860\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.7294 - acc: 0.8777 - val_loss: 0.6544 - val_acc: 0.9015\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.6738 - acc: 0.8871 - val_loss: 0.6158 - val_acc: 0.9063\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.6333 - acc: 0.8953 - val_loss: 0.5636 - val_acc: 0.9176\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.6059 - acc: 0.8989 - val_loss: 0.5695 - val_acc: 0.9104\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5847 - acc: 0.9037 - val_loss: 0.5888 - val_acc: 0.8979\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.5676 - acc: 0.9075 - val_loss: 0.5357 - val_acc: 0.9134\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5524 - acc: 0.9099 - val_loss: 0.5185 - val_acc: 0.9133\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.5402 - acc: 0.9113 - val_loss: 0.4856 - val_acc: 0.9260\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.5272 - acc: 0.9139 - val_loss: 0.4925 - val_acc: 0.9243\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.5174 - acc: 0.9139 - val_loss: 0.4686 - val_acc: 0.9315\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.5092 - acc: 0.9156 - val_loss: 0.5043 - val_acc: 0.9112\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.5018 - acc: 0.9167 - val_loss: 0.4588 - val_acc: 0.9284\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4951 - acc: 0.9177 - val_loss: 0.4474 - val_acc: 0.9367\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4884 - acc: 0.9192 - val_loss: 0.4480 - val_acc: 0.9315\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4830 - acc: 0.9196 - val_loss: 0.4661 - val_acc: 0.9313\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4776 - acc: 0.9201 - val_loss: 0.4456 - val_acc: 0.9328\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4739 - acc: 0.9220 - val_loss: 0.4921 - val_acc: 0.9066\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4669 - acc: 0.9220 - val_loss: 0.4704 - val_acc: 0.9188\n",
      "Test loss: 0.47035287189483643\n",
      "Test accuracy: 0.9188\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_278 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_280 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.04632771093087981\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 2.9204 - acc: 0.8103 - val_loss: 0.8328 - val_acc: 0.8903\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.7958 - acc: 0.8695 - val_loss: 0.7058 - val_acc: 0.8854\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.6716 - acc: 0.8895 - val_loss: 0.5951 - val_acc: 0.9079\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.6115 - acc: 0.8999 - val_loss: 0.5722 - val_acc: 0.9086\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.5785 - acc: 0.9056 - val_loss: 0.5252 - val_acc: 0.9199\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.5538 - acc: 0.9097 - val_loss: 0.5335 - val_acc: 0.9143\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.5367 - acc: 0.9120 - val_loss: 0.4918 - val_acc: 0.9237\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.5206 - acc: 0.9148 - val_loss: 0.5138 - val_acc: 0.9125\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.5071 - acc: 0.9176 - val_loss: 0.5420 - val_acc: 0.9040\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4970 - acc: 0.9190 - val_loss: 0.4592 - val_acc: 0.9294\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4875 - acc: 0.9209 - val_loss: 0.5578 - val_acc: 0.8892\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4781 - acc: 0.9215 - val_loss: 0.4728 - val_acc: 0.9167\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4714 - acc: 0.9231 - val_loss: 0.4891 - val_acc: 0.9117\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4683 - acc: 0.9219 - val_loss: 0.4798 - val_acc: 0.9102\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4619 - acc: 0.9232 - val_loss: 0.4368 - val_acc: 0.9261\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4556 - acc: 0.9258 - val_loss: 0.4207 - val_acc: 0.9296\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4500 - acc: 0.9246 - val_loss: 0.4223 - val_acc: 0.9322\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.4470 - acc: 0.9252 - val_loss: 0.4588 - val_acc: 0.9170\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4405 - acc: 0.9262 - val_loss: 0.4504 - val_acc: 0.9240\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4383 - acc: 0.9265 - val_loss: 0.4877 - val_acc: 0.9093\n",
      "Test loss: 0.4876680865287781\n",
      "Test accuracy: 0.9093\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_281 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.5930269123231967\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 23.0273 - acc: 0.2680 - val_loss: 2.1154 - val_acc: 0.2608\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.0857 - acc: 0.2863 - val_loss: 2.0290 - val_acc: 0.3147\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 2.0220 - acc: 0.3125 - val_loss: 1.9839 - val_acc: 0.3481\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.9783 - acc: 0.3310 - val_loss: 1.9441 - val_acc: 0.3450\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 1.9491 - acc: 0.3383 - val_loss: 2.0294 - val_acc: 0.3245\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.9286 - acc: 0.3416 - val_loss: 1.9014 - val_acc: 0.3668\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.9090 - acc: 0.3491 - val_loss: 1.9102 - val_acc: 0.3653\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8936 - acc: 0.3507 - val_loss: 1.8703 - val_acc: 0.3681\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8824 - acc: 0.3567 - val_loss: 1.8832 - val_acc: 0.3396\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8744 - acc: 0.3554 - val_loss: 1.8899 - val_acc: 0.3802\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8675 - acc: 0.3636 - val_loss: 1.8532 - val_acc: 0.3821\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8618 - acc: 0.3650 - val_loss: 1.8706 - val_acc: 0.3502\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8572 - acc: 0.3652 - val_loss: 1.8645 - val_acc: 0.3923\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 1.8525 - acc: 0.3665 - val_loss: 2.0090 - val_acc: 0.3174\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8483 - acc: 0.3686 - val_loss: 1.8246 - val_acc: 0.3845\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 1.8459 - acc: 0.3679 - val_loss: 1.8188 - val_acc: 0.3801\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8448 - acc: 0.3675 - val_loss: 1.8192 - val_acc: 0.3776\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8403 - acc: 0.3716 - val_loss: 1.8833 - val_acc: 0.3729\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.8398 - acc: 0.3688 - val_loss: 1.8241 - val_acc: 0.3622\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 1.8356 - acc: 0.3694 - val_loss: 1.8469 - val_acc: 0.3828\n",
      "Test loss: 1.8468643051147462\n",
      "Test accuracy: 0.3828\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_284 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.5951791021841185\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 49s 817us/step - loss: 23.2106 - acc: 0.2713 - val_loss: 2.1622 - val_acc: 0.2675\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 2.0893 - acc: 0.2728 - val_loss: 2.0361 - val_acc: 0.2818\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 2.0250 - acc: 0.2873 - val_loss: 1.9886 - val_acc: 0.2969\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 1.9923 - acc: 0.2958 - val_loss: 1.9750 - val_acc: 0.3123\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 1.9669 - acc: 0.3073 - val_loss: 1.9633 - val_acc: 0.2958\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 1.9452 - acc: 0.3172 - val_loss: 1.9269 - val_acc: 0.3408\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 1.9292 - acc: 0.3283 - val_loss: 1.8891 - val_acc: 0.3469\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 1.9141 - acc: 0.3429 - val_loss: 1.9495 - val_acc: 0.3401\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.8912 - acc: 0.3784 - val_loss: 1.9708 - val_acc: 0.3926\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 1.7721 - acc: 0.5073 - val_loss: 1.8886 - val_acc: 0.3994\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 1.6651 - acc: 0.5681 - val_loss: 1.6319 - val_acc: 0.5932\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 1.6087 - acc: 0.5792 - val_loss: 1.5763 - val_acc: 0.5644\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 1.5768 - acc: 0.5851 - val_loss: 1.5773 - val_acc: 0.5900\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 1.5555 - acc: 0.5901 - val_loss: 1.4978 - val_acc: 0.6206\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 1.5411 - acc: 0.5938 - val_loss: 1.4767 - val_acc: 0.6275\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 1.5275 - acc: 0.5964 - val_loss: 1.5166 - val_acc: 0.6144\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 1.5213 - acc: 0.5940 - val_loss: 1.4462 - val_acc: 0.6315\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 1.5128 - acc: 0.5943 - val_loss: 1.4795 - val_acc: 0.5834\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 70us/step - loss: 1.5046 - acc: 0.5954 - val_loss: 1.5091 - val_acc: 0.6070\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 1.4986 - acc: 0.5972 - val_loss: 1.4357 - val_acc: 0.6132\n",
      "Test loss: 1.4357281135559081\n",
      "Test accuracy: 0.6132\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_287 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_288 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_289 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 1.0499511181721517\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 39.2345 - acc: 0.1536 - val_loss: 2.4748 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4762 - acc: 0.1110 - val_loss: 2.4753 - val_acc: 0.1135\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4758 - acc: 0.1116 - val_loss: 2.4753 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4757 - acc: 0.1123 - val_loss: 2.4754 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4756 - acc: 0.1124 - val_loss: 2.4754 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4753 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4751 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4755 - acc: 0.1124 - val_loss: 2.4751 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4751 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4751 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4751 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 2.4754 - acc: 0.1124 - val_loss: 2.4752 - val_acc: 0.1135\n",
      "Test loss: 2.4752485107421873\n",
      "Test accuracy: 0.1135\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_290 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_291 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_292 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.19336228963472718\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 8.5029 - acc: 0.6734 - val_loss: 1.4134 - val_acc: 0.7028\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 1.2353 - acc: 0.7980 - val_loss: 1.1436 - val_acc: 0.8066\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 1.0900 - acc: 0.8195 - val_loss: 1.0226 - val_acc: 0.8366\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 1.0085 - acc: 0.8337 - val_loss: 1.0621 - val_acc: 0.7767\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.9630 - acc: 0.8354 - val_loss: 0.9164 - val_acc: 0.8444\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.9304 - acc: 0.8392 - val_loss: 0.8807 - val_acc: 0.8660\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.8984 - acc: 0.8421 - val_loss: 0.8847 - val_acc: 0.8429\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.8787 - acc: 0.8445 - val_loss: 0.8980 - val_acc: 0.8203\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.8630 - acc: 0.8439 - val_loss: 0.8268 - val_acc: 0.8546\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.8492 - acc: 0.8456 - val_loss: 0.9082 - val_acc: 0.8322\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.8386 - acc: 0.8453 - val_loss: 0.7899 - val_acc: 0.8653\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.8239 - acc: 0.8489 - val_loss: 0.7948 - val_acc: 0.8641\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.8166 - acc: 0.8494 - val_loss: 0.8150 - val_acc: 0.8592\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.8100 - acc: 0.8483 - val_loss: 0.8015 - val_acc: 0.8389\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.8008 - acc: 0.8496 - val_loss: 0.7640 - val_acc: 0.8593\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.7959 - acc: 0.8490 - val_loss: 0.7465 - val_acc: 0.8633\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.7894 - acc: 0.8516 - val_loss: 0.9035 - val_acc: 0.8144\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.7832 - acc: 0.8513 - val_loss: 0.7767 - val_acc: 0.8531\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.7758 - acc: 0.8533 - val_loss: 0.8683 - val_acc: 0.8132\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.7718 - acc: 0.8527 - val_loss: 0.8039 - val_acc: 0.8330\n",
      "Test loss: 0.803895791721344\n",
      "Test accuracy: 0.833\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_293 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_294 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_295 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "training with L2 regularization parameter 0.981797408409048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 36.8536 - acc: 0.1509 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.4648 - acc: 0.1126 - val_loss: 2.4660 - val_acc: 0.1028\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 2.4647 - acc: 0.1117 - val_loss: 2.4645 - val_acc: 0.1135\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 2.4644 - acc: 0.1117 - val_loss: 2.4640 - val_acc: 0.1135\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4642 - val_acc: 0.1135\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4640 - val_acc: 0.1135\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4638 - val_acc: 0.1135\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4640 - val_acc: 0.1135\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4638 - val_acc: 0.1135\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4640 - val_acc: 0.1135\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 2.4642 - acc: 0.1124 - val_loss: 2.4638 - val_acc: 0.1135\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4638 - val_acc: 0.1135\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.4641 - acc: 0.1124 - val_loss: 2.4639 - val_acc: 0.1135\n",
      "Test loss: 2.4639070529937745\n",
      "Test accuracy: 0.1135\n",
      "{'l2': 0.010038202367633864}\n",
      "{'exp_key': None, 'book_time': datetime.datetime(2018, 8, 29, 15, 38, 8, 187000), 'refresh_time': datetime.datetime(2018, 8, 29, 15, 39, 43, 236000), 'owner': None, 'spec': None, 'result': {'status': 'ok', 'loss': -0.9382}, 'tid': 22, 'version': 0, 'misc': {'idxs': {'l2': [22]}, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'tid': 22, 'workdir': None, 'vals': {'l2': [0.010038202367633864]}}, 'state': 2}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    model_2 = Sequential()\n",
    "    model_2.add(Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=regularizers.l2(params['l2'])))\n",
    "    # model_2.add(Dropout(0.2))\n",
    "    model_2.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(params['l2'])))\n",
    "    # model_2.add(Dropout(0.2))\n",
    "    model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_2.summary()\n",
    "    \n",
    "    print('training with L2 regularization parameter', params['l2'])\n",
    "    \n",
    "    model_2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history_2 = model_2.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    score_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    print('Test loss:', score_2[0])\n",
    "    print('Test accuracy:', score_2[1])\n",
    "\n",
    "    return {'loss': score_2[0], 'status': STATUS_OK} \n",
    "\n",
    "space = {\n",
    "    'l2': hp.uniform('l2', 0.0001, 5)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmax(objective, space, algo=tpe.suggest, trials=trials, max_evals = 30)\n",
    "print (best)\n",
    "print (trials.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqIAAAGPCAYAAACH27KGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAG4ZJREFUeJzt3W9snAed4PHf44mb4sBSO84C6sSepUlbtqQkNEnjRZSN2C4nXlwrutnoRJOikoStRFecOYl9gXScDrqVQD4h8mIhuymbjZT1nlpVESCxbbR7ZXWFkCOBbYuaBjF2DAmQ2AlK3D/OzNwLlkATJ7Hj8fxi8/lIkZg+T/z85pnHzDfPzDxTNBqNRgAAQIu1ZQ8AAMDvJiEKAEAKIQoAQAohCgBACiEKAEAKIQoAQAohCgBACiEKAEAKIQoAQAohCgBAigXZA0zHwoULY8mSJdljAABwCb/4xS/i1VdfndK6cypElyxZEiMjI9ljAABwCeVyecrremkeAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAUQhQAgBQLsgfgd0Oj0YgDQ2NRPXE2Kt2LYnVvZxRF0fLttGqOmc55rf7sa2mb0zXVGefCfZmvLtz3d/TcEP9v+FTq7/NsbWO+HWczuT+t3hczfZ7Iep6Z7OdGxCW3Va/XY/d3huO5n5yOd9745rj/zp5oa7v2zj8WjUajkT3EVJXL5RgZGckeg2kaGRuPzTv3x9HR8WgvtcVErR5Luzpi14Nro9zZ0bLttGqOmc55rf7sa2mb0zXVGefCfZmvLtz3r52rnX9Czfp9nq1tzLfjbCb3p9X7YqbPE1nPM5P93Le++foooohjp1++aFvHT78S/2XHt2Oi9pvEay8VsWfrulhd6WrGrrys6fSaEGVWNRqNeP/A/4mhk+NRq//mUCu1FVFZ3BFP97+vaf9SvNx2nvqvd8Wf/K9nZn2Omc45kzlata+ztzldU51xLtyX+epS+/5Crfx9nq3jYb4dZzO5P63eFzN9npjp8mYfM5P59baGTp6Nc/WLl7eXinjxf/6nWT8zOp1eu/bO0TKvHBgai5HRly/65anVGzE8Oh4HhsZasp3d3xluyRwznXMmc7RqX2dvc7qmOuNcuC/z1aX2/YVa+fs8W8fDfDvOZnJ/Wr0vZvo8MdPlzT5mJlOrN6J6cnzSCI2ImKg1Yvd3hq9qjtkiRJlV1RNnY0Fp8n8BtpfaonribEu289xPTrdkjiuZzf3Rqn2dvc3pmuqMc+G+zFeX2/cXatXv82wdD/PtOJvJ/Wn1vpjp88RMl8/GMTOZK6353E9OX9Ucs0WIMqsq3Ytiojb5P80mavWodC9qyXbeeeObWzLHlczm/mjVvs7e5nRNdca5cF/mq8vt+wu16vd5to6H+XaczeT+tHpfzPR5YqbLZ+OYmcyVzpu+88Y3X9Ucs0WIMqtW93bG0q6OKLW9/t9opbYiero6zn/qb7a3c/+dPS2ZY6ZzzmSOVu3r7G1O11RnnAv3Zb661L6/UCt/n2freJhvx9lM7k+r98VMnydmurzZx8xkfv0e0QWXqLv2UhH339lzVXPMFiHKrCqKInY9uDZ6F3dEe6mIjutK0V761S/Kro/e2bQ3ol9pO21tbS2ZY6ZzzmSOVu3r7G3O1oxz4b7MV5Pt+wVtv3rSzPp9nq3jYb4dZzO5P63eFzN9npjp8mYfMz1db4iersm39Y/b+qL9gpfz20tF/OO2vmvuEk4+NU9LuI7o9Oa8Vn/2tbTN6XId0Wuf64jO3ePMdURdR/S3uXwTAAApXL4JAIBrnhAFACCFEAUAIIUQBQAghRAFACCFEAUAIIUQBQAghRAFACCFEAUAIIUQBQAghRAFACCFEAUAIIUQBQAghRAFACCFEAUAIIUQBQAghRAFACCFEAUAIIUQBQAghRAFACCFEAUAIIUQBQAghRAFACCFEAUAIEVTQrRer8fDDz8cN910Uyxbtiy2b98+6XqvvPJK3HvvvXHzzTfHu971rrj77rvjyJEjzRgBAIA5pikhunv37njhhRfi8OHDsX///vj85z8fzz///KTrbtu2LV588cX4/ve/H/fcc09s2bKlGSMAADDHNCVEBwcHY+vWrVEqlaKrqys2btwYe/bsuWi966+/Pj74wQ9GURQREbFu3bqoVqvNGAEAgDmmKSE6PDwcvb29529XKpUYHh6+4t/74he/GPfcc88llw8MDES5XD7/58yZM80YFwCAa8CCqazU19cXL7300qTLDh48eFUbfuSRR+LIkSOxb9++S67T398f/f3952+Xy+Wr2hYAANeeKYXos88+e9nlPT09MTQ0FH19fRERUa1Wo6en55Lrf+ELX4gnnnginn766ejo6JjGuAAAzBdNeWl+w4YNsWPHjqjVajE6OhqDg4OxcePGSdcdGBiIPXv2xFNPPRU33HBDMzYPAMAc1JQQ3bRpU9x6662xfPnyWLNmTfT398eKFSsiImLv3r3nPxk/MjISn/zkJ+PUqVOxfv36WLlyZdx5553NGAEAgDmmaDQajewhpqpcLsfIyEj2GAAAXMJ0es03KwEAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQYkH2AFzbGo1GHBgai+qJs1HpXhSrezujKIrssQCAeUCIckkjY+Oxeef+ODo6Hu2ltpio1WNpV0fsenBtlDs7sscDAOY4L80zqUajEZt37o+hk+MxUWvE+Gu1mKg1YujkeDywc380Go3sEQGAOU6IMqkDQ2MxMvpy1OqvD85avRHDo+NxYGgsaTIAYL4QokyqeuJsLChN/l7Q9lJbVE+cbfFEAMB8I0SZVKV7UUzU6pMum6jVo9K9qMUTAQDzjRBlUqt7O2NpV0eU2l5/VrTUVkRPV0es7u1MmgwAmC+aEqL1ej0efvjhuOmmm2LZsmWxffv2K/6dxx57LIqiiCeffLIZI9BkRVHErgfXRu/ijmgvFdFxXSnaS0VUFnfEro/e6RJOAMCMNeXyTbt3744XXnghDh8+HKdPn45Vq1bF+vXr47bbbpt0/Wq1Gjt27Ih169Y1Y/PMknJnR+zrf5/riAIAs6IpZ0QHBwdj69atUSqVoqurKzZu3Bh79uyZdN16vR5btmyJL33pS7Fw4cJmbJ5ZVBRFrKl0xYbVS2NNpUuEAgBN05QQHR4ejt7e3vO3K5VKDA8PT7ruwMBAvOc974k77rijGZsGAGCOmtJL8319ffHSSy9NuuzgwYNT3thzzz0Xjz/+eDzzzDNTWn9gYCAGBgbO3z5z5syUtwUAwLVtSiH67LPPXnZ5T09PDA0NRV9fX0T86j2gPT09F633rW99K6rVaixfvjwiIo4fPx7btm2LY8eOxUMPPXTR+v39/dHf33/+drlcnsq4AADMAUWjCd/V+NWvfjX+4R/+If75n//5/IeVvva1r8WKFSsu+/f++I//OD7xiU/EvffeO6XtlMvlGBkZmem4AADMkun0WlPeI7pp06a49dZbY/ny5bFmzZro7+8/H6F79+6NLVu2NGMzAADMI005I9oqzogCAFzbWn5GFAAApkuIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkEKIAgCQQogCAJBCiAIAkKIpIVqv1+Phhx+Om266KZYtWxbbt2+/5LqvvvpqfPzjH4/ly5fHihUr4v7772/GCAAAzDELmvFDdu/eHS+88EIcPnw4Tp8+HatWrYr169fHbbfddtG6f/VXfxVFUcThw4ejKIo4fvx4M0YAAGCOacoZ0cHBwdi6dWuUSqXo6uqKjRs3xp49ey5a7+zZs/F3f/d38bnPfS6KooiIiLe+9a3NGAEAgDmmKSE6PDwcvb29529XKpUYHh6+aL0f/ehH0dXVFY888kisXr063vve98a+ffsu+XMHBgaiXC6f/3PmzJlmjAsAwDVgSi/N9/X1xUsvvTTpsoMHD055Y+fOnYuhoaH4wz/8w3j00Ufj4MGDcffdd8fzzz8fb3nLWy5av7+/P/r7+8/fLpfLU94WzdFoNOLA0FhUT5yNSveiWN3bef5sNgDATEwpRJ999tnLLu/p6YmhoaHo6+uLiIhqtRo9PT2TrtfW1hYf/vCHIyJi1apV8Qd/8Afx7//+75OGKLlGxsZj8879cXR0PNpLbTFRq8fSro7Y9eDaKHd2ZI8HAMxxTXlpfsOGDbFjx46o1WoxOjoag4ODsXHjxovW6+7ujve///3xzW9+MyIifvzjH8ePf/zjeMc73tGMMWiiRqMRm3fuj6GT4zFRa8T4a7WYqDVi6OR4PLBzfzQajewRAYA5rikhumnTprj11ltj+fLlsWbNmujv748VK1ZERMTevXtjy5Yt59f9m7/5m/j85z8fK1asiHvvvTe+/OUvx4033tiMMWiiA0NjMTL6ctTqrw/OWr0Rw6PjcWBoLGkyAGC+KBpz6NRWuVyOkZGR7DF+J/zvA0fjv+99PsZfq120rOO6UvyP/3xbbFi9NGEyAOBaNp1e881KTKrSvSgmavVJl03U6lHpXtTiiQCA+UaIMqnVvZ2xtKsjSm2v/4R8qa2Inq6OWN3bmTQZADBfCFEmVRRF7HpwbfQu7oj2UhEd15WivVREZXFH7PronS7hBADMWFO+4pP5qdzZEfv63+c6ogDArBCiXFZRFLGm0hVrKl3ZowAA84yX5gEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASCFEAQBIIUQBAEghRAEASNGUEK3X6/Hwww/HTTfdFMuWLYvt27dfct1vfOMb8e53vztWrlwZ73znO+Pv//7vmzECAABzzIJm/JDdu3fHCy+8EIcPH47Tp0/HqlWrYv369XHbbbe9br1GoxH3339//Ou//mvcfvvtUa1W49Zbb40PfehD8aY3vakZowAAMEc05Yzo4OBgbN26NUqlUnR1dcXGjRtjz549k65bFEWcOnUqIiJ++ctfxuLFi2PhwoXNGAMAgDmkKWdEh4eHo7e39/ztSqUS3/72ty9aryiKGBwcjA996EOxaNGiGBsbiyeeeCKuu+66SX/uwMBADAwMnL995syZZowLAMA1YEpnRPv6+qK7u3vSP0ePHp3yxs6dOxef/exn44knnoihoaHYt29fbNq0KU6cODHp+v39/TEyMnL+zxvf+MYpbwsAgGvblM6IPvvss5dd3tPTE0NDQ9HX1xcREdVqNXp6ei5a79ChQ/HTn/407rrrroiIWLNmTZTL5Th48GDcfffd050dAIA5rCnvEd2wYUPs2LEjarVajI6OxuDgYGzcuPGi9ZYuXRrHjh2LH/7whxERceTIkfjRj34Ut9xySzPGAABgDmnKe0Q3bdoU3/3ud2P58uVRFEX09/fHihUrIiJi7969sXfv3vjbv/3beMtb3hJf+cpX4s///M+jra0t6vV6bN++fdKzpwAAzG9Fo9FoZA8xVeVyOUZGRrLHAADgEqbTa75ZCQCAFEIUAIAUQhQAgBRN+bASU9NoNOLA0FhUT5yNSveiWN3bGUVRZI8FAJBCiLbIyNh4bN65P46Ojkd7qS0mavVY2tURux5cG+XOjuzxAABazkvzLdBoNGLzzv0xdHI8JmqNGH+tFhO1RgydHI8Hdu6POXThAgCAphGiLXBgaCxGRl+OWv31wVmrN2J4dDwODI0lTQYAkEeItkD1xNlYUJr8vaDtpbaonjjb4okAAPIJ0RaodC+KiVp90mUTtXpUuhe1eCIAgHxCtAVW93bG0q6OKLW9/qxoqa2Inq6OWN3bmTQZAEAeIdoCRVHErgfXRu/ijmgvFdFxXSnaS0VUFnfEro/e6RJOAMDvJJdvapFyZ0fs63+f64gCAPwHIdpCRVHEmkpXrKl0ZY8CAJDOS/MAAKQQogAApBCiAACkEKIAAKQQogAApBCiAACkEKIAAKQQogAApBCiAACkEKIAAKQQogAApBCiAACkEKIAAKQQogAApBCiAACkEKIAAKQQogAApBCiAACkEKIAAKRYkD3AXNRoNOLA0FhUT5yNSveiWN3bGUVRZI8FADCnCNEp+nV8Hhwei6/+32r8/JevxHULSjFRq8fSro7Y9eDaKHd2ZI8JADBnCNEpGBkbj80798fR0fE4V2tE4z/++7nXahERMXRyPB7YuT+e7n+fM6MAAFPkPaJX0Gg0YvPO/TF0cjwmfitCf1ut3ojh0fE4MDTW8vkAAOYqIXoFB4bGYmT05ajVJ0vQ32gvtUX1xNkWTQUAMPcJ0SuonjgbC0pXfrl9olaPSveiFkwEADA/CNErqHQviola/bLrlNqK6OnqiNW9nS2aCgBg7hOiV7C6tzOWXuHT8L1db4hdH73TB5UAAKZBiF5BURTx3z5wyyWXL2gr4tH7bo8bb3hDC6cCAJj7hOgUnH31XLyhffJddd2Cthg6Od7iiQAA5j4hOgWV7kVx7hKfmvchJQCAqyNEp2B1b2cs7eqIUtvr3wPqQ0oAAFdPiE5BURSx68G10bu4I9pLRXRcV4r2UhGVxR0+pAQAcJV8xecl/Pq75asnzkale1Gs7u2Mff3vu+i/iVAAgKsjRCfx298t315qi4laPZZ2dcSuB9fGmkpXrKl0ZY8IADDneWn+Ahd+t/z4a7WYqDVi6OR4PLBzfzQal/+qTwAApkaIXuBS3y1fqzdieHQ8DgyNJU0GADC/CNELXO675dtLbVE9cbbFEwEAzE9C9AKX+2551wwFAGgeIXoB1wwFAGgNIXoB1wwFAGgNl2+aRLmzwzVDAQBmmRC9hKIoXDMUAGAWeWkeAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAURaPRaGQPMVULFy6MJUuWtGRbZ86ciTe+8Y0t2Razy2M5P3gc5weP4/zgcZwfZutx/MUvfhGvvvrqlNadUyHaSuVyOUZGRrLHoAk8lvODx3F+8DjODx7H+eFaeBy9NA8AQAohCgBAitJnPvOZz2QPca3q6+vLHoEm8VjODx7H+cHjOD94HOeH7MfRe0QBAEjhpXkAAFIIUQAAUghRAABSCNFJvPTSS/FHf/RHcfPNN8eaNWvi+eefzx6JafrLv/zLqFQqURRFHDp0KHscrtIrr7wS9957b9x8883xrne9K+6+++44cuRI9lhchT/90z+N22+/PVauXBnvfe974+DBg9kjMQOPPfZYFEURTz75ZPYoXIVKpRK33HJLrFy5MlauXBmDg4NpswjRSXzsYx+Lbdu2xeHDh+NTn/pUfOQjH8keiWn6sz/7s/i3f/u36O3tzR6FGdq2bVu8+OKL8f3vfz/uueee2LJlS/ZIXIV/+qd/ih/84Adx6NCh6O/v9/+rc1i1Wo0dO3bEunXrskdhBgYHB+PQoUNx6NCh2LhxY9ocQvQCP//5z+PAgQNx//33R0TEfffdF0ePHnUWZo656667olwuZ4/BDF1//fXxwQ9+MIqiiIiIdevWRbVazR2Kq3LDDTec/9+nT58+/5gyt9Tr9diyZUt86UtfioULF2aPwzywIHuAa83Ro0fjbW97WyxY8KtdUxRF9PT0xPDwcCxbtix5Ovjd9sUvfjHuueee7DG4Sps3b45/+Zd/iYiIb3zjG8nTcDUGBgbiPe95T9xxxx3ZozBDmzdvjkajEWvXro1HH300lixZkjKHM6LAnPDII4/EkSNH4q//+q+zR+Eq7dq1K44ePRqf/exn41Of+lT2OEzTc889F48//nh8+tOfzh6FGXrmmWfiBz/4QXzve9+L7u7ueOCBB9JmcUb0AkuXLo1jx47FuXPnYsGCBdFoNGJ4eDh6enqyR4PfWV/4whfiiSeeiKeffjo6Ojqyx2GGHnjggfiLv/iLOHnyZCxevDh7HKboW9/6VlSr1Vi+fHlERBw/fjy2bdsWx44di4ceeih5Oqbj103T3t4en/jEJ+Lmm29Om8UZ0Qv8/u//frz73e+O3bt3R0TE448/HuVy2cvykGRgYCD27NkTTz311OveZ8jccerUqfjpT396/vaTTz4Zixcvjq6ursSpmK6HHnoojh07FtVqNarVaqxbty6+8pWviNA55uzZs3Hq1Knzt/fs2ROrVq1Km8cZ0Ul8+ctfjo985CPxyCOPxO/93u/FY489lj0S0/Sxj30svv71r8fx48fjAx/4QLzpTW/ygbM5aGRkJD75yU/G29/+9li/fn1ERCxcuDC+853vJE/GdJw+fTo2bNgQL7/8crS1tcWSJUvia1/7mg8sQYKf/exncd9990WtVotGoxFvf/vbY9euXWnz+K55AABSeGkeAIAUQhQAgBRCFACAFEIUAIAUQhQAgBRCFACAFEIUAIAUQhQAgBT/HxWkVuQMNRLLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure(num=None, figsize=(10, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "t = trials.trials\n",
    "\n",
    "y = []\n",
    "x = []\n",
    "for tr in t: \n",
    "    y.append((tr['result']['loss']))\n",
    "    x.append(tr['misc']['vals']['l2'])\n",
    "    \n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_203 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 2.6406 - acc: 0.8173 - val_loss: 0.7989 - val_acc: 0.8758\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.7571 - acc: 0.8758 - val_loss: 0.7895 - val_acc: 0.8406\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.6485 - acc: 0.8958 - val_loss: 0.6333 - val_acc: 0.8861\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.5874 - acc: 0.9053 - val_loss: 0.5580 - val_acc: 0.9082\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.5479 - acc: 0.9124 - val_loss: 0.5196 - val_acc: 0.9170\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.5222 - acc: 0.9149 - val_loss: 0.5336 - val_acc: 0.9105\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.5023 - acc: 0.9192 - val_loss: 0.4545 - val_acc: 0.9361\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.4892 - acc: 0.9195 - val_loss: 0.4447 - val_acc: 0.9343\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.4770 - acc: 0.9216 - val_loss: 0.4519 - val_acc: 0.9306\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4678 - acc: 0.9240 - val_loss: 0.4344 - val_acc: 0.9310\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4575 - acc: 0.9241 - val_loss: 0.4312 - val_acc: 0.9275\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.4504 - acc: 0.9258 - val_loss: 0.4480 - val_acc: 0.9227\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.4419 - acc: 0.9274 - val_loss: 0.4096 - val_acc: 0.9349\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.4388 - acc: 0.9272 - val_loss: 0.4407 - val_acc: 0.9205\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.4322 - acc: 0.9278 - val_loss: 0.4093 - val_acc: 0.9314\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.4277 - acc: 0.9290 - val_loss: 0.4617 - val_acc: 0.9130\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.4240 - acc: 0.9272 - val_loss: 0.4486 - val_acc: 0.9163\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.4195 - acc: 0.9298 - val_loss: 0.3766 - val_acc: 0.9424\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.4171 - acc: 0.9286 - val_loss: 0.3861 - val_acc: 0.9375\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.4120 - acc: 0.9300 - val_loss: 0.4288 - val_acc: 0.9292\n",
      "Test loss: 0.4288289388179779\n",
      "Test accuracy: 0.9292\n"
     ]
    }
   ],
   "source": [
    "l2_min = trials.best_trial['misc']['vals']['l2']\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=regularizers.l2(l2_min)))\n",
    "# model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_min)))\n",
    "# model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_2 = model_2.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score_2[0])\n",
    "print('Test accuracy:', score_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAGPCAYAAACDLmK7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XtwVOd9//HP2ZWELS414lIrWS0qlTcakDFIyKFKE3GrA5kix1U9zjQUNkYWxPWUWqVoJm5TknYo7kX9Ma0zUWzQFIixccW4hF6mwfHYxk4CGhlRq8RIHa9215YtCwlUSQZJu+f3B5VqVbJZoeU5e3m/ZnbG+DzWfp85ePej89ws27ZtAQAAAIa4nC4AAAAA6YUACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjMpwuIBYzZszQggULnC4DAAAAn+DDDz/UtWvXYmqbFAF0wYIFCofDTpcBAACAT+DxeGJuyxA8AAAAjCKAAgAAwCgCKAAAAIwigAIAAMAoAigAAACMIoACAADAKAIoAAAAjCKAAgAAwCgCKAAAAIwigAIAAMAoAigAAACMIoACAADAKAIoAAAAjCKAAgAAwKiYA2hbW5vKysrk8/lUWlqq1tbWCW2i0ah27dqloqIiFRYWatu2bRoaGpIkBQIBud1uLV++fOz1X//1X/HrCQAAAJJCzAF0+/btqq6u1sWLF1VbWyu/3z+hzYEDB9Tc3Kzm5mZduHBBLpdL+/fvH7s+e/ZsnTt3buz1q7/6q3HpBAAAAJJHTAG0q6tLTU1N2rx5sySpsrJSoVBI7e3t49q1tLRo/fr1ysrKkmVZ2rhxow4fPhz/qgEAAJC0YgqgoVBIubm5ysjIkCRZliWv16tgMDiuXUlJiU6cOKG+vj4NDw/r2LFjCgQCY9cHBgZUWlqq4uJiffe731UkEolfTwAAAJAU4roIye/3a8OGDSovL1d5ebl8Pt9YaM3NzdW7776rs2fP6tSpU3rttdf0N3/zN5P+nLq6Onk8nrFXf39/PMsEAACAgyzbtu0bNerq6lJBQYF6enqUkZEh27aVm5ur06dPq6Cg4BP/u+eee05PPfWUXnvttQnXjh49qmeffVY/+tGPblikx+NROBy+YTsAAAA4Yyp5LaYnoAsXLlRxcbGOHDkiSWpsbJTH45kQPq9evare3l5JUnd3t/bt26fdu3dLuh5ih4eHJUnXrl3T8ePHtWLFith6BAAAgJSREWvD+vp6+f1+7d27V3PmzFFDQ4MkqaqqShUVFaqoqNCVK1e0evVquVwuRaNR7dy5U5s2bZIknT59Wt/+9rfldrs1MjKitWvX6oknnrg1vQIAAEDCimkI3mkMwQMAACS2uA/BAwAAAPFCAAUAAIBRBFAAAAAYRQAFAACAUQRQAAAAGEUABQAAgFEEUAAAABhFAAUAAIBRBFAAAAAYRQAFAACAUQRQAAAAGEUABQAAgFEEUAAAABhFAAUAAIBRBFAAAAAYRQAFAACAUQRQAAAAGEUABQAAgFEEUAAAABhFAAUAAIBRBFAAAAAYRQAFAACAUQRQAAAAGJXhdAFILrZtq6mjV4HuAeXPn6mVi+bKsiyny5qWVOwTAACJjACKmIV7B7Xl4BmFegaV6XZpOBJVXk62Dj18rzxzs50u76akYp8AAEh0DMEjJrZta8vBM+q4NKjhiK3BoYiGI7Y6Lg1q68Ezsm3b6RKnLBX7BABIb7Zt62ygRy80hXQ20JOw32U8AUVMmjp6Fe75SJHo+L/IkaitYM+gmjp6VZqf41B1NycV+wQASF/JNKrHE1DEJNA9oAz35PMiM90uBboHDFc0fanYJwBAekq2UT0CKGKSP3+mhiPRSa8NR6LKnz/TcEXTl4p9AgCkp1hG9RIJARQxWblorvJysuV2jX9i6HZZ8uZka+WiuQ5VdvNSsU8AAOc5MQ8z2Ub1mAOKmFiWpUMP3zthbok3J1uHtn0+KbctSsU+IT2xlRiQOJyah5lso3qWnWiTAibh8XgUDoedLgNKzS+6VOwT0kcyLToAUp1t21pX94o6Lg2OGwp3uyzlz8vWqZryW/b94uR7j5pKXmMIHlNiWZZK83P04Mo8lebnpERQS8U+IT0k26IDINU5OQ9zdFRv0bxsZbotZWe5lem+Hj4TcVSPIXgASFJsJQYkltF5mEORiddG52Heyv8nPXOz9VJNeVKM6sX8BLStrU1lZWXy+XwqLS1Va2vrhDbRaFS7du1SUVGRCgsLtW3bNg0NDU1o5/f7ZVmWLl++PL3qASCNJduiAyDVJcI8zGQZ1Ys5gG7fvl3V1dW6ePGiamtr5ff7J7Q5cOCAmpub1dzcrAsXLsjlcmn//v3j2hw/flyZmZnTLhwA0l0ifNkB+F/srhK7mAJoV1eXmpqatHnzZklSZWWlQqGQ2tvbx7VraWnR+vXrlZWVJcuytHHjRh0+fHjs+gcffKC9e/eqrq4ujl0AgPTElx2QWJJtHqaTYpoDGgqFlJubq4yM680ty5LX61UwGFRBQcFYu5KSEtXX1+uxxx7T7bffrmPHjikQCIxdf+SRR/SXf/mXmj17dnx7AQBpKBG3EmNXCaS7ZJqH6aS4LkLy+/3q6OhQeXm5br/9dq1fv17//u//Lkl65pln5PV6tXbt2hv+nLq6unFPSfv7++NZJhAzvkyR6BLpy44toYDrRudhsgjwk8W0D2hXV5cKCgrU09OjjIwM2bat3NxcnT59etwT0P/rueee01NPPaXXXntNX//61/Xqq6/K7XZLkjo6OpSXl6d/+qd/0ooVKz71/dkHFE7gyxSIXSLsQQjAWXHfB3ThwoUqLi7WkSNHJEmNjY3yeDwTwufVq1fV23t9j6vu7m7t27dPu3fvliT98Ic/VCgUUiAQGBuWP3/+/A3DJ+AE9lcEpibZzqEG4KyYV8HX19ervr5ePp9P+/btU0NDgySpqqpKJ06ckCRduXJFZWVlWrp0qb74xS9qx44d2rRp062pHLiF+DIFpoYtoQBMRcxzQD/3uc/ppz/96YR//8wzz4z98y//8i/rwoULMf08niAhkTm9mTCQbNgSCsBUcBQnMAm+TIGpYUsoAFNBAAUmwZcpMDXsfwhgKmJaBe80VsHDCZOtgh/dX/Gzd9zudHlAQmLrMiB9TSWvEUCBT8GXKQAAsZlKXovrRvRAqmEzYQAA4o8ACgBJjif1AJINARRIUYSS9MCJXQCSEQEUSEGEkvTw8RO7IlFbw5HrG9eOntjF8ZcAEhXbMAEphmNE0wcndgFIVgRQIMUQStIHx18CSFYEUCDFEErSByd2AUhWBFBMi23bOhvo0QtNIZ0N9DC8mwAIJemDE7sAJCsWIeGmsdAlMY2GktGFKaMIJaln9PjLTzqxiwVIABIVJyHhpti2rXV1r0wacvLnZbP61mEcI5pe2HIL08XfIcQDJyHhlotloQunBznHMzdbL9WU84WSJjixC9PBaBacwBxQ3BQWuiS+0VDy4Mo8lebnED4BTMC2bXAKAfRjWFATOxa6AEDyY9s2OIUh+P/BEMTUsNAFAJLf6GjWUGTitdHRLKZ24FbgCagYgrgZo6tvF83LVqbbUnaWW5nu6wuQWH0LAMmB0Sw4hSegYkHNzWKhCwAkN0az4BSegIoFNdPBQhcASF6MZsEpPAEVQxAAgPTFaBacQAAVQxAAgPTGXrIwjSF4MQQBAABgEk9A/wdDEAAAAGYQQD8m1YcgOOsXAAAkAgJommCjfQAAkCiYA5oG2GgfiC+O7QWA6eEJaBpgo30gfhhNAIDp4wloGmCjfSA+GE0AgPgggKYBNtoH4iOW0QQAwI0RQNPA6Eb7btf4p6BstA9MDaMJyYN5ukBiYw5oGhjdaP//zlvz5rDRPjAVjCYkB+bpAomPAJom2GgfmD6O7U18H5+nG4naGo5EJGlsnu6pmnI+94AEEPMQfFtbm8rKyuTz+VRaWqrW1tYJbaLRqHbt2qWioiIVFhZq27ZtGhoakiS98847Kikp0fLly1VUVKQHH3xQvb3MlzJpdKP9B1fmqTQ/hw/hOGGoLzGYuA8c25v4mKcLJAfLjvFTeu3atdqyZYv8fr/+8R//UU8++aTOnj07rs3TTz+to0eP6t/+7d+UmZmp6upq+Xw+/dEf/ZGuXbumaDSq22+/XZK0c+dOSdL+/ftv+N4ej0fhcHiqfQNuOYb6EoPp+8CpYonrhaaQ/vREqwaHIhOuZWe59Z2KpXpwZZ4DlQGpbyp5LaYnoF1dXWpqatLmzZslSZWVlQqFQmpvbx/XrqWlRevXr1dWVpYsy9LGjRt1+PBhSdKMGTPGwmckEtHAwAAf2EhqbMmTGJy4D4wmJC7m6QLJIaYAGgqFlJubq4yM61NGLcuS1+tVMBgc166kpEQnTpxQX1+fhoeHdezYMQUCgbHrQ0NDWr58uebPn6+2tjZ95zvfmfT96urq5PF4xl79/f032T3g1mGoLzFwH/Bx7PoBJIe4bsPk9/u1YcMGlZeXq7y8XD6fbyy0SlJWVpbOnTunDz74QIWFhaqvr5/059TU1CgcDo+9Zs2aFc8ygbhgS57EwH3AxzFPF0gOMa2Cz8vLU2dnp0ZGRpSRkSHbthUMBuX1ese1syxLe/bs0Z49eyRJzz33nJYuXTrh52VlZekb3/iGHnnkEe3evXv6vQAcwFBfYuA+4P9i1w8g8cX0BHThwoUqLi7WkSNHJEmNjY3yeDwqKCgY1+7q1atjK9u7u7u1b9++sYDZ0dGhwcFBSddXy7/wwgtatmxZ3DoCmJZIQ33pvBI/ke4DEgfzdIHEFvMq+Lffflt+v1+XLl3SnDlz1NDQoLvvvltVVVWqqKhQRUWFPvjgA61evVoul0vRaFQ7d+7Ujh07JEk/+tGP9MQTT0i6HkCLi4v1t3/7t5o3b94N35tV8EhUk62+Ht3g/7N33O5YDem2Ej8R7gMApLup5LWYA6iTCKBIZE5uyWPbttbVvTLpxuj587LTatNttkYCAGdNJa9xEhIwTaNDfaX5OcbfO5YV4E7U5QQn7wMAYGriugoegFmsAAcAJCOegMYRQ4AwjRXgwP/iMxhIHgTQOGEhCJwwugJ8sjmgrABHOuEzGEguDMHHAUcywilsug3wGQwkI56AxgELQeAkNt1GuuMzGEg+PAGNAxaCwGlsuo10xmcwkHx4AhoHLAQB0hOLXhIDn8FA8iGAxgELQYD0w6KXxMFncHzxixVM4CSkOOEoQCB9cAJV4uEzOD74xQrTwVGcDuG3RiA9nA306OtP/1xDkwz7ZrotPfvIKha9OIDP4OnhFytMF0dxOoSjAIH0MLroZSgy8droohc+B8zjM3h62E0AJrEKHgCmiEUvSEXvfNivT3rAyW4CiDcCKABM0eiiF7dr/Lc1i16QrMK9g/p/L7Xp2gi/WMEMAigATBEnUCGVjJ4k9UHftUmvuyzxixXijjmgAHATOIEKqeKT5n6OuvOXbuMXK8QdARQAbhKLXpAKPm1R3YwMlx5f72MrK8QdQ/AAAKSxT1tUF7Vt5n7iliCAAgCQxlhUBycQQAEASGMsqoMTmAMKAECaY1EdTCOAAgAAFtXBKIbgAQAAYBQBFAAAAEYRQAEAAGAUc0ABjLFtm0UIAIBbjgAKQJIU7h3UloNnFOoZVKbbpeFIVHk52Tr08L3yzM12ujwAQAphCB6AbNvWloNn1HFpUMMRW4NDEQ1HbHVcGtTWg2dk25OfEQ0AwM0ggAJQU0evwj0fKRIdHzQjUVvBnkE1dfQ6VBkAIBURQAEo0D2gDPfkcz0z3S4FugcMVwQASGUEUADKnz9Tw5HopNeGI1Hlz59puCIAQCojgALQykVzlZeTLbdr/FNQt8uSNydbKxfNdagyAEAqIoACkGVZOvTwvVo0L1uZbkvZWW5lui3lz8vWoW2fZysmAEBcsQ0TAEmSZ262XqopZx9QAMAtF/MT0La2NpWVlcnn86m0tFStra0T2kSjUe3atUtFRUUqLCzUtm3bNDQ0JEn6j//4D33pS19SYWGhioqK9PDDD+ujjz6KX08ATJtlWSrNz9GDK/NUmp9D+AQA3BIxB9Dt27erurpaFy9eVG1trfx+/4Q2Bw4cUHNzs5qbm3XhwgW5XC7t379fknTbbbfp7//+7/WLX/xCLS0tGhgY0JNPPhm3jgAAACA5xBRAu7q61NTUpM2bN0uSKisrFQqF1N7ePq5dS0uL1q9fr6ysLFmWpY0bN+rw4cOSpLvuukvLli2TJLndbpWWlioQCMSxKwAAAFNn27bOBnr0QlNIZwM9HL5hQEwBNBQKKTc3VxkZ16eMWpYlr9erYDA4rl1JSYlOnDihvr4+DQ8P69ixY5OGzIGBAT3zzDO6//77J32/uro6eTyesVd/f/8UuwUAAHBj4d5Brat7Rb/z9M/0pyda9TtP/0zr6l5RuHfQ6dJSWlxXwfv9fm3YsEHl5eUqLy+Xz+cbC62jhoaG9NBDD+m+++7TAw88MOnPqampUTgcHnvNmjUrnmUCAABwDLGDYgqgeXl56uzs1MjIiKTrNywYDMrr9Y5rZ1mW9uzZozfffFNvvPGGlixZoqVLl45dHx4e1kMPPaTc3NyxuaEAAABO4Bhi58QUQBcuXKji4mIdOXJEktTY2CiPx6OCgoJx7a5evare3us3q7u7W/v27dPu3bslSSMjI/ra176mnJwc/eAHP2B1LQAAcBTHEDsn5n1A6+vr5ff7tXfvXs2ZM0cNDQ2SpKqqKlVUVKiiokJXrlzR6tWr5XK5FI1GtXPnTm3atEmS9Pzzz+v48eNatmyZVqxYIUn6whe+oKeeeuoWdAsAAODTcQyxcyw7CSY4eDwehcNhp8sAAAApxLZtrat7RR2XBscNw7td10+CO1VTzojtFEwlr3EUJwAASEscQ+wcjuIEAABpi2OInUEABQAAaW30GOLS/BynS0kbDMEDAADAKAIoAAAAjCKAAgAAwCjmgCIt2LbNBHMAABIEARQpL9w7qC0HzyjUM6hMt0vDkajycrJ16OF75Zmb7XR5AACkHYbgkdJs29aWg2fUcWlQwxFbg0MRDUdsdVwa1NaDZ5QE5zAAAJByCKBIaU0dvQr3fDTuhAtJikRtBXsG1dTR61BlAACkLwIoUlqge0AZ7snnema6XQp0DxiuCAAAEECR0vLnz9RwJDrpteFIVPnzZxquCAAAEECR0lYumqu8nGy5XeOfgrpdlrw52Vq5aK5DlQEAkL4IoEhplmXp0MP3atG8bGW6LWVnuZXptpQ/L1uHtn2erZgAAHAA2zAh5XnmZuulmnL2AQUAIEEQQJEWLMtSaX6OSvNznC4FAIC0xxA8AAAAjOIJKAAASDocsZzcCKAAACCpcMRy8mMIHgAAJA2OWE4NBFAAAJA0OGI5NRBAAQBA0uCI5dRAAAUAAEmDI5ZTAwEUAAAkDY5YTg0EUAAAkDQ4Yjk1sA0TAABIKhyxnPwIoAAAIOlwxHJyYwgeAAAARhFAAQAAYBQBFAAAAEYRQAEAAGAUARQAAABGEUABAABgFAEUAAAARsUcQNva2lRWViafz6fS0lK1trZOaBONRrVr1y4VFRWpsLBQ27Zt09DQkCSpv79fX/7ylzV//nzdcccd8esBAAAAkkrMAXT79u2qrq7WxYsXVVtbK7/fP6HNgQMH1NzcrObmZl24cEEul0v79++XJGVmZqq2tlanTp2KW/EAAABIPjEF0K6uLjU1NWnz5s2SpMrKSoVCIbW3t49r19LSovXr1ysrK0uWZWnjxo06fPiwJGnGjBlau3YtTz8BAADSXEwBNBQKKTc3VxkZ10/utCxLXq9XwWBwXLuSkhKdOHFCfX19Gh4e1rFjxxQIBKZcVF1dnTwez9irv79/yj8DAAAAiSmui5D8fr82bNig8vJylZeXy+fzjYXWqaipqVE4HB57zZo1K55lAgAAwEExBdC8vDx1dnZqZGREkmTbtoLBoLxe77h2lmVpz549evPNN/XGG29oyZIlWrp0afyrBgAAQNKKKYAuXLhQxcXFOnLkiCSpsbFRHo9HBQUF49pdvXpVvb29kqTu7m7t27dPu3fvjnPJAAAASGYxj4/X19fL7/dr7969mjNnjhoaGiRJVVVVqqioUEVFha5cuaLVq1fL5XIpGo1q586d2rRp09jPWLZsmT788EP19fXJ4/FozZo1Y4uUAAAAkB4s27Ztp4u4EY/Ho3A47HQZAAAA+ARTyWuchAQAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMCrmANrW1qaysjL5fD6VlpaqtbV1QptoNKpdu3apqKhIhYWF2rZtm4aGhsaunzx5UoWFhbrrrrv0W7/1W+rr64tPLwAAAJA0Yg6g27dvV3V1tS5evKja2lr5/f4JbQ4cOKDm5mY1NzfrwoULcrlc2r9/vySpv79f27Zt04svvqi2tjZ95jOf0Z/92Z/FrSMAAABIDjEF0K6uLjU1NWnz5s2SpMrKSoVCIbW3t49r19LSovXr1ysrK0uWZWnjxo06fPiwJOlf//VftWLFChUWFkqSHn30UR09ejSefQEAAEASiCmAhkIh5ebmKiMjQ5JkWZa8Xq+CweC4diUlJTpx4oT6+vo0PDysY8eOKRAISJKCwaAWLVo01jY/P1+dnZ0aGRmZ8H51dXXyeDxjr/7+/pvtHwAAABJMXBch+f1+bdiwQeXl5SovL5fP5xsLrVNRU1OjcDg89po1a1Y8ywQAAICDYgqgeXl5455W2ratYDAor9c7rp1lWdqzZ4/efPNNvfHGG1qyZImWLl0qSfJ6vero6BhrGwgExj1VBQAAQHqIKYAuXLhQxcXFOnLkiCSpsbFRHo9HBQUF49pdvXpVvb29kqTu7m7t27dPu3fvliRt2LBBzc3N+sUvfiFJ+t73vqevfe1rcesIAAAAkkPMjx/r6+vl9/u1d+9ezZkzRw0NDZKkqqoqVVRUqKKiQleuXNHq1avlcrkUjUa1c+dObdq0SZI0e/ZsPfPMM/rqV7+qkZERFRUV6R/+4R9uTa8AAACQsCzbtm2ni7gRj8ejcDjsdBkAAAD4BFPJa5yEBAAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKg9gBJATbttXU0atA94Dy58/UykVzZVmW02UBAG4BAigAx4V7B7Xl4BmFegaV6XZpOBJVXk62Dj18rzxzs50uDwAQZwzBA3CUbdvacvCMOi4Najhia3AoouGIrY5Lg9p68IyS4LRgIC5s29bZQI9eaArpbKCHv/tIaTwBBeCopo5ehXs+UiQ6/ss2ErUV7BlUU0evSvNzHKoOMINRAKQbnoACcFSge0AZ7snnema6XQp0DxiuCDCLUQCkIwIoAEflz5+p4Uh00mvDkajy5880XBFgViyjAECqIYACcNTKRXOVl5Mtt2v8U1C3y5I3J1srF811qDLADEYBkI4IoAAcZVmWDj18rxbNy1am21J2lluZbkv587J1aNvn2YoJKY9RAKQjFiEBcJxnbrZeqilnH1CkpdFRgI5Lg+OG4RkFQCrjCSiAhGBZlkrzc/TgyjyV5ucQPpE2GAVAOuIJKAAADmMUAOmGAAoAQAIYHQVg31ukA4bgAQAAYBQBFAAAAEYRQAEAAGAUARQAAABGEUABAABgFAEUAAAARhFAAQAAYBQBFAAAAEYRQAEAAGAUARQAAABGEUABAABgFAEUAAAARhFAAQAAYBQBFAAAAEbFHEDb2tpUVlYmn8+n0tJStba2TmgTjUZVU1OjJUuWaNmyZVqzZo3a29vHrv/VX/2VioqKtGTJEj3wwAO6fPlyfHoBAACApBFzAN2+fbuqq6t18eJF1dbWyu/3T2hz4sQJvf7662ppadH58+e1bt06fetb35Ik/fjHP1ZDQ4N++tOf6j//8z9VUlKiJ554Im4dAQAAQHKIKYB2dXWpqalJmzdvliRVVlYqFAqNe7opSZZl6dq1a7p69aps21ZfX588Ho8kqaWlRb/+67+u2bNnS5K+8pWv6PDhw/HsCwAAAJJATAE0FAopNzdXGRkZkq4HTa/Xq2AwOK7dpk2btHr1at15553Kzc3VSy+9pO9+97uSpJKSEp06dUrvv/++bNvWD3/4Q/33f/+3enp6JrxfXV2dPB7P2Ku/v3+6/QQAAECCiOsipKamJr311lt699139d5772ndunXasWOHJGnNmjXatWuXfvM3f1OrVq3SggULJGks1H5cTU2NwuHw2GvWrFnxLBMAAAAOmpj+JpGXl6fOzk6NjIwoIyNDtm0rGAzK6/WOa3fo0CGtXbtWd9xxhyRp69atuu+++8auP/roo3r00UclST/72c/k8Xg0Z86cePUFAAAASSCmJ6ALFy5UcXGxjhw5IklqbGyUx+NRQUHBuHaLFy/WT37yEw0NDUmSTp48qaKiorHrnZ2dkqTBwUF9+9vf1u7du+PSCQAAACQPy7ZtO5aGb7/9tvx+vy5duqQ5c+aooaFBd999t6qqqlRRUaGKigpdu3ZNjz32mE6fPq3MzEzdeeed+v73v6/FixdLku6++25Fo1ENDQ3pd3/3d/Unf/Insizrhu/t8XgUDoen11MAAADcMlPJazEHUCcRQAEAABLbVPIaJyEBAADAKAIoAAAAjCKAAgAAwCgCKAAAAIwigAIAAMAoAigAAACMIoACAADAKAIoAAAAjCKAAgAAwCgCKAAAAIwigAIAAMAoAigAAACMIoACAADAKAIoAAAAjCKAAgAAwCgCKAAAAIwigAKjr1JiAAAJIElEQVQAAMAoAigAAACMIoACAADAKAIoAAAAjCKAAgAAwCgCKAAAAIwigAIAAMCoDKcLAAAkB9u21dTRq0D3gPLnz9TKRXNlWZbTZQFIQgRQAMANhXsHteXgGYV6BpXpdmk4ElVeTrYOPXyvPHOznS4PQJJhCB4A8Kls29aWg2fUcWlQwxFbg0MRDUdsdVwa1NaDZ2TbttMlAkgyBFAAwKdq6uhVuOcjRaLjg2YkaivYM6imjl6HKgOQrAigAIBPFegeUIZ78rmemW6XAt0DhisCkOwIoACAT5U/f6aGI9FJrw1HosqfP9NwRQCSHQEUAPCpVi6aq7ycbLld45+Cul2WvDnZWrlorkOVAUhWBFAAwKeyLEuHHr5Xi+ZlK9NtKTvLrUy3pfx52Tq07fNsxQRgytiGCQBwQ5652Xqpppx9QAHEBQEUABATy7JUmp+j0vwcp0sBkOQYggcAAIBRMQfQtrY2lZWVyefzqbS0VK2trRPaRKNR1dTUaMmSJVq2bJnWrFmj9vb2setPPvmklixZouXLl2vVqlU6c+ZMfHoBAACApBFzAN2+fbuqq6t18eJF1dbWyu/3T2hz4sQJvf7662ppadH58+e1bt06fetb35IknTt3Tt/73vd05swZnTt3To899pgee+yxuHUEAAAAySGmANrV1aWmpiZt3rxZklRZWalQKDTu6aZ0fX7QtWvXdPXqVdm2rb6+Pnk8nrFrw8PDGhi4vmHx5cuXx64BAAAgfcS0CCkUCik3N1cZGdebW5Ylr9erYDCogoKCsXabNm3Syy+/rDvvvFOzZ8/WZz/7Wb3yyiuSpHvuuUePP/64fuVXfkU5OTmaMWOGXn311Unfr66uTnV1dWN/7u/vv+kOAgAAILHEdRFSU1OT3nrrLb377rt67733tG7dOu3YsUOS9M477+j48eNqb29XOBzW448/roceemjSn1NTU6NwODz2mjVrVjzLBAAAgINiCqB5eXnq7OzUyMiIJMm2bQWDQXm93nHtDh06pLVr1+qOO+6Qy+XS1q1b9fLLL0uSGhsbdffdd+szn/mMJOkb3/iGXn/9dQ0NDcWzPwAAAEhwMQXQhQsXqri4WEeOHJF0PUx6PJ5xw++StHjxYv3kJz8ZC5UnT55UUVHR2LXXX399bDj95MmT8vl8ysrKiltnAAAAkPhi3oi+vr5efr9fe/fu1Zw5c9TQ0CBJqqqqUkVFhSoqKvR7v/d7unDhgu655x5lZmbqzjvv1Pe//31J0gMPPKCzZ89q5cqVmjFjhmbOnKlnn3321vQKAAAACcuybdt2uogb8Xg8CofDTpcBAACATzCVvMZJSAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMIoAAAADCKAAoAAACjCKAAAAAwigAKAAAAowigAAAAMIoACgAAAKMs27Ztp4u4kRkzZmjBggUxt+/v79esWbNuYUUwjXuaerinqYn7mnq4p6nnVt3TDz/8UNeuXYupbVIE0KnyeDwKh8NOl4E44p6mHu5pauK+ph7uaepJhHvKEDwAAACMIoACAADAKPeePXv2OF3ErfBrv/ZrTpeAOOOeph7uaWrivqYe7mnqcfqepuQcUAAAACQuhuABAABgFAEUAAAARhFAAQAAYFRKBdC2tjaVlZXJ5/OptLRUra2tTpeEafr93/995efny7IsnTt3zulyEAdXr17VV7/6Vfl8Pt1zzz36jd/4DbW3tztdFqbpvvvu07Jly7R8+XJ98Ytf1Jtvvul0SYiThoYGWZalF1980elSEAf5+fn63Oc+p+XLl2v58uV6/vnnHakjpQLo9u3bVV1drYsXL6q2tlZ+v9/pkjBNv/3bv63Tp09r0aJFTpeCOKqurtbbb7+tlpYW3X///aqqqnK6JEzTsWPHdP78eZ07d041NTV8/qaIQCCgp59+WqtWrXK6FMTR888/r3PnzuncuXN66KGHHKkhZQJoV1eXmpqatHnzZklSZWWlQqEQT1aS3Je+9CV5PB6ny0Ac3XbbbfrKV74iy7IkSatWrVIgEHC2KEzbHXfcMfbPV65cGbu/SF7RaFRVVVX6u7/7O82YMcPpcpBiMpwuIF5CoZByc3OVkXG9S5Zlyev1KhgMqqCgwOHqAHyS/fv36/7773e6DMTBli1b9PLLL0uS/uVf/sXhajBddXV1+sIXvqCSkhKnS0GcbdmyRbZt695779W+ffu0YMEC4zWkzBNQAMln7969am9v11/8xV84XQri4NChQwqFQvrzP/9z1dbWOl0OpuGtt95SY2Oj/viP/9jpUhBnr776qs6fP6/m5mbNnz9fW7dudaSOlHkCmpeXp87OTo2MjCgjI0O2bSsYDMrr9TpdGoBJ/PVf/7WOHz+uU6dOKTs72+lyEEdbt27Vjh07dOnSJc2bN8/pcnATXnvtNQUCAd11112SpPfff1/V1dXq7OzUN7/5TYerw3SM5qLMzEz9wR/8gXw+nyN1pMwT0IULF6q4uFhHjhyRJDU2Nsrj8TD8DiSguro6HT16VD/+8Y/HzR1Ecrp8+bLee++9sT+/+OKLmjdvnnJychysCtPxzW9+U52dnQoEAgoEAlq1apV+8IMfED6T3MDAgC5fvjz256NHj2rFihWO1JIyT0Alqb6+Xn6/X3v37tWcOXPU0NDgdEmYpu3bt+uf//mf9f777+vLX/6yZs+ezcKyJBcOh/WHf/iHWrx4sdasWSNJmjFjhn7+8587XBlu1pUrV/Tggw/qo48+ksvl0oIFC3Ty5EkWIgEJ5oMPPlBlZaUikYhs29bixYt16NAhR2rhLHgAAAAYlTJD8AAAAEgOBFAAAAAYRQAFAACAUQRQAAAAGEUABQAAgFEEUAAAABhFAAUAAIBRBFAAAAAY9f8BKx3bjGFmQssAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 2.9080 - acc: 0.8543 - val_loss: 1.0653 - val_acc: 0.8932\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.9618 - acc: 0.9093 - val_loss: 0.8665 - val_acc: 0.9237\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.8502 - acc: 0.9228 - val_loss: 0.8073 - val_acc: 0.9308\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.7992 - acc: 0.9294 - val_loss: 0.7849 - val_acc: 0.9319\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.7698 - acc: 0.9340 - val_loss: 0.7956 - val_acc: 0.9234\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.7497 - acc: 0.9361 - val_loss: 0.7307 - val_acc: 0.9401\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.7337 - acc: 0.9380 - val_loss: 0.7198 - val_acc: 0.9371\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.7220 - acc: 0.9402 - val_loss: 0.6973 - val_acc: 0.9446\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.7104 - acc: 0.9419 - val_loss: 0.7270 - val_acc: 0.9332\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.7007 - acc: 0.9426 - val_loss: 0.6864 - val_acc: 0.9464\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6920 - acc: 0.9457 - val_loss: 0.6771 - val_acc: 0.9473\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6847 - acc: 0.9464 - val_loss: 0.6891 - val_acc: 0.9445\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.6781 - acc: 0.9475 - val_loss: 0.6665 - val_acc: 0.9495\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6727 - acc: 0.9479 - val_loss: 0.6567 - val_acc: 0.9522\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6681 - acc: 0.9484 - val_loss: 0.6616 - val_acc: 0.9494\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6629 - acc: 0.9497 - val_loss: 0.6498 - val_acc: 0.9535\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6596 - acc: 0.9505 - val_loss: 0.6729 - val_acc: 0.9447\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6548 - acc: 0.9522 - val_loss: 0.6506 - val_acc: 0.9514\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6519 - acc: 0.9518 - val_loss: 0.6701 - val_acc: 0.9434\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.6483 - acc: 0.9519 - val_loss: 0.6398 - val_acc: 0.9529\n",
      "Test loss: 0.6397903204917907\n",
      "Test accuracy: 0.9529\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "# model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "# model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_3.summary()\n",
    "\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_3 = model_3.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score_3 = model_3.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_3[0])\n",
    "print('Test accuracy:', score_3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 2.9117 - acc: 0.8523 - val_loss: 1.0735 - val_acc: 0.8984\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.9683 - acc: 0.9058 - val_loss: 0.8827 - val_acc: 0.9142\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.8699 - acc: 0.9174 - val_loss: 0.8444 - val_acc: 0.9207\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.8241 - acc: 0.9244 - val_loss: 0.7782 - val_acc: 0.9340\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7899 - acc: 0.9277 - val_loss: 0.7454 - val_acc: 0.9360\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7689 - acc: 0.9301 - val_loss: 0.7463 - val_acc: 0.9355\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.7567 - acc: 0.9315 - val_loss: 0.7346 - val_acc: 0.9349\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7448 - acc: 0.9337 - val_loss: 0.7301 - val_acc: 0.9380\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7337 - acc: 0.9357 - val_loss: 0.7309 - val_acc: 0.9342\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7237 - acc: 0.9382 - val_loss: 0.7088 - val_acc: 0.9419\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7164 - acc: 0.9391 - val_loss: 0.7356 - val_acc: 0.9314\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7078 - acc: 0.9418 - val_loss: 0.6963 - val_acc: 0.9418\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.7031 - acc: 0.9406 - val_loss: 0.6965 - val_acc: 0.9413\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6977 - acc: 0.9422 - val_loss: 0.6938 - val_acc: 0.9410\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6920 - acc: 0.9427 - val_loss: 0.6746 - val_acc: 0.9465\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6892 - acc: 0.9425 - val_loss: 0.6705 - val_acc: 0.9468\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6855 - acc: 0.9427 - val_loss: 0.6638 - val_acc: 0.9470\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6799 - acc: 0.9440 - val_loss: 0.6691 - val_acc: 0.9465\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6779 - acc: 0.9437 - val_loss: 0.6656 - val_acc: 0.9446\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.6741 - acc: 0.9447 - val_loss: 0.6619 - val_acc: 0.9491\n",
      "Test loss: 0.661853397655487\n",
      "Test accuracy: 0.9491\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=regularizers.l1_l2(l1=0.001,l2=0.001)))\n",
    "#model_4.add(Dropout(0.2))\n",
    "model_4.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001,l2=0.001)))\n",
    "#model_4.add(Dropout(0.2))\n",
    "model_4.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_4.summary()\n",
    "\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_4 = model_4.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score_4 = model_4.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_4[0])\n",
    "print('Test accuracy:', score_4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history_2.history['acc'])\n",
    "plt.plot(history_2.history['val_acc'])\n",
    "plt.plot(history_3.history['acc'])\n",
    "plt.plot(history_3.history['val_acc'])\n",
    "plt.plot(history_4.history['acc'])\n",
    "plt.plot(history_4.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_1', 'test_1', 'train_2', 'test_2', 'train_3', 'test_3', 'train_4', 'test_4'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.plot(history_3.history['loss'])\n",
    "plt.plot(history_3.history['val_loss'])\n",
    "plt.plot(history_4.history['loss'])\n",
    "plt.plot(history_4.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_1', 'test_1', 'train_2', 'test_2', 'train_3', 'test_3', 'train_4', 'test_4'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
